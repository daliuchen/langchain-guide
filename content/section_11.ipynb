{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 结构化结果和工具调用\n",
    "\n",
    "## 结构化结果\n",
    "LLM返回结构化数据这一功能极具实用价值，例如从数据中提取接口参数，亦或是将数据存储于数据库。\n",
    "接下来介绍几种相关方法\n",
    "\n",
    "### with_structured_output方法\n",
    "这是模型所提供的原生 API 用于输出结构化数据，此方式是最为简单且可靠的。不管是工具调用、函数调用还是 json 模型，其底层实现均采用这种模式。\n",
    "有一部分模型并不支持这一方法，而支持的 LLM 如下所示：\n",
    "https://python.langchain.com/v0.2/docs/integrations/chat/"
   ],
   "id": "c63105bec726d60d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:36:10.950397Z",
     "start_time": "2024-07-24T15:36:05.862280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# 开启debug\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "eb36d8e81819b265",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以看到上面在模型的返回中`text`是没有值的，在`kwargs.additional_kwargs.tool_calls`中返回了json，按照既定格式要求返回了数据。LangSmith如下：\n",
    "\n",
    "![](../resource/img_13.png)\n",
    "可以看到，这里将模型转为了`函数调用`，所以，这里的实现还是 function tools,查看`with_structured_output`源码可以证明\n",
    "![](../resource/img_14.png)"
   ],
   "id": "17a29cb2b1088e15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "也可以将schema的定义通过json的形式传递来，这样返回值是dict",
   "id": "a23ea9229006895c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:58:15.143852Z",
     "start_time": "2024-07-24T13:58:12.017764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "6d94e1e61cf0e91e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e31081af085992a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 在多个返回值类型中选择\n",
    "\n",
    "可以规定多个类型，让模型在里面做选择，如下所示"
   ],
   "id": "9f38649bc5fc0a13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:50:43.006165Z",
     "start_time": "2024-07-24T15:50:40.252442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: Union[Joke, ConversationalResponse]\n",
    "\n",
    " \n",
    "structured_llm = llm.with_structured_output(Response)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "\n",
    "# 选择了 Joke"
   ],
   "id": "9d6418f7e697291a",
   "execution_count": 74,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:00:32.487734Z",
     "start_time": "2024-07-24T14:00:30.171617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 选择了ConversationalResponse\n",
    "structured_llm.invoke(\"How are you today?\")"
   ],
   "id": "b23125cfe1301fea",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Streaming\n",
    "也支持流式输出"
   ],
   "id": "e3477936094013cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:04:50.325220Z",
     "start_time": "2024-07-24T14:04:47.447596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Response)\n",
    "set_debug(False)\n",
    "\n",
    "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
    "    print(chunk,type(chunk))"
   ],
   "id": "593caf639b99c82b",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Few-shot prompting\n",
    "\n",
    "对于更复杂的模式，向提示添加少量示例非常有用。这可以通过几种方式来实现。\n",
    "最简单的方式是将Example直接添加到system中"
   ],
   "id": "a29c740b056b432b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:09:46.382193Z",
     "start_time": "2024-07-24T14:09:43.120584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "set_debug(True)\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n",
    "\n",
    "Here are some examples of jokes:\n",
    "\n",
    "example_user: Tell me a joke about planes\n",
    "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
    "\n",
    "example_user: Tell me another joke about planes\n",
    "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
    "\n",
    "example_user: Now about caterpillars\n",
    "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
   ],
   "id": "d6d956e00b6fdc32",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (Advanced) Specifying the method for structuring outputs\n",
    "\n",
    "是用`json_mode`的形式，需要在promot中指定JSON返回值，并且schema的key。\n",
    "`with_structured_output`支持两种模型，一种是`function_calling` 如上所述，一种是`json_mode`, 在`json_mode`下，不会将schema包装为function tools给llm，而是一个promot，所以需要在promot中规定返回的json格式。源码和LangSmith如下\n",
    "![](../resource/img_15.png)\n",
    "![](../resource/img_16.png)"
   ],
   "id": "6959d28ada761acd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:00:31.380207Z",
     "start_time": "2024-07-24T16:00:28.904709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")"
   ],
   "id": "5969a05d0ee82fb8",
   "execution_count": 75,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (Advanced) Raw outputs\n",
    "\n",
    "LLMs 在生成结构化输出时并非尽善尽美，尤其是当模式趋于复杂的时候。您可以通过传递 include_raw=True 这一参数，以避免引发异常，并自行去处理原始输出。这样做将会改变输出的格式，其中会包含原始消息输出（raw）、解析后的值（若成功解析）（parsed）以及任何产生的错误（parsing_error）。"
   ],
   "id": "2e7679d41864e159"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:28:27.968313Z",
     "start_time": "2024-07-24T14:28:24.954923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Joke, include_raw=True)\n",
    "\n",
    "res = structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")\n",
    "print(res)"
   ],
   "id": "9df5ff8af88c817a",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:29:11.303645Z",
     "start_time": "2024-07-24T14:29:11.299944Z"
    }
   },
   "cell_type": "code",
   "source": "res.keys()",
   "id": "a26553b1bb35bf90",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:29:46.702710Z",
     "start_time": "2024-07-24T14:29:46.700242Z"
    }
   },
   "cell_type": "code",
   "source": "res[\"parsed\"]",
   "id": "1070b98d4c87fc9a",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 自己写promot，自己解析输出值\n",
    "\n",
    "并非所有模型都支持 .with_structured_output() 这一功能，原因在于并非所有模型都支持工具调用或 JSON 模式。对于这类模型，你的直接提示模型采用特定的格式，同时使用输出解析器从原始的模型输出里提取结构化的响应。\n",
    "\n",
    "##### PydanticOutputParser"
   ],
   "id": "715d55029083309f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:01.158717Z",
     "start_time": "2024-07-24T14:44:01.153657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ],
   "id": "633df1b2e4187bf",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:05.853566Z",
     "start_time": "2024-07-24T14:44:05.850342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt_template.invoke(query).to_string())"
   ],
   "id": "216a4c039a2ca0e9",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:11.750681Z",
     "start_time": "2024-07-24T14:44:08.516265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代入llm\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "365d892432844285",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Custom Parsing\n",
    "\n",
    "还可以使用 LangChain 表达式语言（LCEL）创建自定义提示和解析器，通过一个普通函数来解析来自模型的输出。\n",
    "这种方式是llm刚开始的阶段，不推荐使用。"
   ],
   "id": "ef943b614dd78ea6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:49.427704Z",
     "start_time": "2024-07-24T14:44:49.421740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Output your answer as JSON that  \"\n",
    "            \"matches the given schema: ```json\\n{schema}\\n```. \"\n",
    "            \"Make sure to wrap the answer in ```json and ``` tags\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(schema=People.schema())\n",
    "\n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"```json(.*?)```\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ],
   "id": "78621524d3c15d32",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:45:21.079061Z",
     "start_time": "2024-07-24T14:45:21.076716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.format_prompt(query=query).to_string())"
   ],
   "id": "90d4a378f9774004",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:09:18.500548Z",
     "start_time": "2024-07-24T16:09:13.145728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | llm | extract_json\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "7efef5c3a9b92976",
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 如何使用chat models来调用工具\n",
    "\n",
    "官方文档：\n",
    "https://python.langchain.com/v0.2/docs/how_to/custom_tools/#creating-tools-from-functions\n",
    "\n",
    "### 将tools传递给llm\n",
    "\n",
    "`@tool`用来定义工具， 在定义的时候需要通过类型参数，文档注释等等来声明工具的作用，名字，参数，返回只等，在默认状况下，此装饰器会把函数名当作工具名。"
   ],
   "id": "e859ee7db766aff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:20:29.241212Z",
     "start_time": "2024-07-24T16:20:29.233896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "print(type(add))\n",
    "print(add.name)\n",
    "print(add.args)\n",
    "print(add.description)"
   ],
   "id": "d83d3cf48ebaad26",
   "execution_count": 86,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:18:34.941059Z",
     "start_time": "2024-07-24T16:18:32.000946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 声明工具，传递给llm\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "ai_msg = llm_with_tools.invoke(query)"
   ],
   "id": "cc2d1facd8783470",
   "execution_count": 84,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**要注意：LLM本身没有调用函数的能力，他的返回值只是告诉我们需要调用函数，调用哪些函数，入参是什么。**\n",
    "这些信息都封装在`tool_calls`方法中"
   ],
   "id": "7cef66e83de847c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:20:05.519449Z",
     "start_time": "2024-07-24T16:20:05.516101Z"
    }
   },
   "cell_type": "code",
   "source": "ai_msg.tool_calls",
   "id": "ed4b1c5d1c680c28",
   "execution_count": 85,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "说回到tool,他的类型是`StructuredTool`，他也是实现了`Runnable`接口，所以，他是可以直接通过`invoke`调用的， 可以直接在LCEL中使用的",
   "id": "5412e7175f3d9810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:21:56.274001Z",
     "start_time": "2024-07-24T16:21:56.268584Z"
    }
   },
   "cell_type": "code",
   "source": "add.invoke({\"a\":1,\"b\":1})",
   "id": "214b40c8711d49a1",
   "execution_count": 87,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "okk，对于上面的结果，就可以找到对应的tool，将llm中的调用tool的参数塞进去调用就完事。",
   "id": "522d173e8a139e80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:23:01.687653Z",
     "start_time": "2024-07-24T15:23:01.684941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call[\"args\"])\n",
    "    print(tool_msg)"
   ],
   "id": "242fdbdd6b156ab8",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "end",
   "id": "58ebc14b740f796a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
