{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chatbotsä¸€äº›ç›¸å…³é—®é¢˜\n",
    "Chatbotä½¿ç”¨äº†LLMåšå¯¹è¯ï¼Œä¸‹é¢çš„åˆ—ä¸¾äº†ç›¸å…³é—®é¢˜\n",
    "\n",
    "## å¦‚ä½•ç®¡ç†å¯¹è¯è®°å¿†\n",
    "èŠå¤©æœºå™¨äººçš„ç‰¹ç‚¹æ˜¯å¯ä»¥å°†ä¹‹å‰å¯¹è¯çš„å†…å®¹ç”¨æˆ·å½“å‰ä¸Šä¸‹æ–‡ã€‚åœ¨ä¹‹å‰æˆ‘ä»¬è®²è¿‡ï¼Œæœ‰å¤šç§æ–¹å¼å¯ä»¥åšï¼Œä½†ä¹‹å‰ä½¿ç”¨çš„ä¸æ˜¯LCELè¡¨è¾¾å¼ã€‚ä¸‹é¢å±•ç¤ºä½¿ç”¨LCELçš„å®ç°\n"
   ],
   "id": "6ebd7b61123bdfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "bfb57b29eada5f47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:24.569633Z",
     "start_time": "2024-08-07T12:40:21.413446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"ä½ æ˜¯ä»€ä¹ˆ\",\n",
    "            ),\n",
    "            (\"ai\", \"æˆ‘æ˜¯AIå°åŠ©æ‰‹\"),\n",
    "            (\"human\", \"åˆšæ‰é—®ä½ ä»€ä¹ˆäº†?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.content)"
   ],
   "id": "a0d47645a21f4a68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ é—®æˆ‘\"ä½ æ˜¯ä»€ä¹ˆ\"\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ChatHistory\n",
    "LangChainæä¾›äº† `ChatMessageHistory`æ¥è®°å½•å¯¹è¯å†å²ï¼ŒåŸºæœ¬ä½¿ç”¨å¦‚ä¸‹ï¼š"
   ],
   "id": "6dfc641578c541ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:30.001566Z",
     "start_time": "2024-08-07T12:40:29.997347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\"\n",
    ")\n",
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"ä½ æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    ")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"æˆ‘æ˜¯AIå°åŠ©æ‰‹\")\n",
    "demo_ephemeral_chat_history.messages"
   ],
   "id": "3decaa89fca1b126",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜'),\n",
       " HumanMessage(content='ä½ æ˜¯ä»€ä¹ˆï¼Ÿ'),\n",
       " AIMessage(content='æˆ‘æ˜¯AIå°åŠ©æ‰‹')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸æ­¤åŒæ—¶ï¼Œåœ¨åœ¨ä¿å­˜AI responseçš„æ—¶å€™ä¸éœ€è¦ä¸Šé¢çš„æ“ä½œï¼Œä»–å°è£…äº†æ“ä½œï¼Œå¯ä»¥ç›´æ¥å°†response saveèµ·æ¥",
   "id": "de2055adb87e8af6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:36.192818Z",
     "start_time": "2024-08-07T12:40:32.145911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "input1 = \"ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\"\n",
    "demo_ephemeral_chat_history.add_user_message(input1)\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "input2 = \"åˆšæ‰é—®ä½ ä»€ä¹ˆäº†ï¼Ÿ\"\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(input2)\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")"
   ],
   "id": "eb0a5d0bdbf1da8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ é—®æˆ‘ï¼šâ€œä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜â€ï¼Œæˆ‘å›ç­”è¯´ï¼šâ€œæ²¡é—®é¢˜ï¼è¯·é—®ä½ æœ‰ä»€ä¹ˆé—®é¢˜éœ€è¦å¸®åŠ©çš„å—ï¼Ÿâ€', response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 92, 'total_tokens': 144}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b8985b10-2518-413c-b605-1f005c522983-0', usage_metadata={'input_tokens': 92, 'output_tokens': 52, 'total_tokens': 144})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²\n",
    "\n",
    "LangChainæä¾›äº†è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²çš„ç±»`RunnableWithMessageHistory`,ä¸ç”¨åƒä¸Šé¢ä¸€æ ·æ‰‹åŠ¨ç®¡ç†äº†"
   ],
   "id": "fc46df5502e162ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:38.418321Z",
     "start_time": "2024-08-07T12:40:38.414922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat"
   ],
   "id": "c0ca74df13fc837b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:46.167961Z",
     "start_time": "2024-08-07T12:40:46.162924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ],
   "id": "f1e6a16a78130490",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:51.635912Z",
     "start_time": "2024-08-07T12:40:48.654749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"æˆ‘é¥¿äº†\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ],
   "id": "94e8f5afd213dc59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='é‚£ä½ å¯ä»¥è€ƒè™‘åƒç‚¹ä¸œè¥¿æ¥å¡«é¥±è‚šå­ï¼Œå¯ä»¥é€‰æ‹©å¥åº·çš„é£Ÿç‰©ï¼Œæ¯”å¦‚æ°´æœã€è”¬èœã€åšæœæˆ–è€…ä¸€äº›è½»é£Ÿã€‚å¦‚æœéœ€è¦æˆ‘å¸®ä½ æ‰¾ä¸€äº›ç®€å•çš„é£Ÿè°±æˆ–è€…å¤–å–å¹³å°çš„æ¨èï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„å£å‘³åå¥½ã€‚å¸Œæœ›ä½ èƒ½æ‰¾åˆ°æ»¡è¶³çš„é£Ÿç‰©ï¼', response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 36, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d5c40320-035f-4a0d-97ae-e8cf3354b571-0', usage_metadata={'input_tokens': 36, 'output_tokens': 115, 'total_tokens': 151})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:40:57.591517Z",
     "start_time": "2024-08-07T12:40:55.857577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_with_message_history.invoke(\n",
    "    {\"input\": \"åˆšæ‰é—®ä½ ä»€ä¹ˆäº†ï¼Ÿ\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
    ")"
   ],
   "id": "ef86700447370c70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ è¯´ä½ é¥¿äº†ï¼Œæˆ‘å»ºè®®ä½ åƒç‚¹ä¸œè¥¿æ¥å¡«é¥±è‚šå­ã€‚å¦‚æœéœ€è¦æˆ‘å†å¸®ä½ æŸ¥æ‰¾ä¸€äº›é£Ÿè°±æˆ–è€…å¤–å–æ¨èï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼', response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 170, 'total_tokens': 229}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c70a506e-9db4-4281-8bc9-31b8db55fb18-0', usage_metadata={'input_tokens': 170, 'output_tokens': 59, 'total_tokens': 229})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ç®¡ç†å¯¹è¯å†å²\n",
    "éšç€å¯¹è¯æ¬¡æ•°çš„è¶Šæ¥è¶Šå¤šï¼Œå¯¹è¯å†å²ä¹Ÿå°±è¶Šæ¥è¶Šé•¿ï¼Œå¦‚æœä¸å¤„ç†ï¼Œå°±ä¼šè¶…è¿‡æ¨¡å‹çš„Contextï¼Œå¹¶ä¸”è¾“å…¥çš„Contextè¿‡é•¿ï¼Œä¼šå¯¹æ¨¡å‹é€ æˆäº›å¹²æ‰°ã€‚\n",
    "\n",
    "åœ¨è¿™ç¯‡æ–‡ç« ä¸­æœ‰è¯¦ç»†çš„é˜è¿°ï¼Œåœ¨è¿™é‡Œæˆ‘åˆ—ä¸¾ä¸¤ä¸ª\n",
    "https://python.langchain.com/v0.2/docs/how_to/trim_messages/"
   ],
   "id": "29cf36fe7c0ce861"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Trimming messages\n",
    "\n",
    "è¿™ä¸ªè§£å†³æ–¹æ¡ˆæ˜¯å°†å†å²æ¶ˆæ¯ä¼ é€’ç»™å¤§æ¨¡å‹ä¹‹å‰ç°åœ¨åšä¸€æ¬¡é¢„å¤„ç†ï¼Œä½¿ç”¨äº†LCELè¡¨è¾¾å¼è¯­æ³•"
   ],
   "id": "c136470de73639d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:45:58.870524Z",
     "start_time": "2024-08-07T12:45:58.866175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "demo_ephemeral_chat_history.add_user_message(\"helloï¼Œæˆ‘æ˜¯Ethan\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"å¥½ç€å‘¢ï¼Ÿ\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ],
   "id": "294f51703a6df274",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='helloï¼Œæˆ‘æ˜¯Ethan'),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ'),\n",
       " AIMessage(content='å¥½ç€å‘¢ï¼Ÿ')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:46:09.692507Z",
     "start_time": "2024-08-07T12:46:09.687956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "\n",
    "\n",
    "chain_with_trimming = (\n",
    "    RunnablePassthrough.assign(chat_history=itemgetter(\"chat_history\") | trimmer)\n",
    "    | prompt\n",
    "    | chat\n",
    ")\n",
    "\n",
    "chain_with_trimmed_history = RunnableWithMessageHistory(\n",
    "    chain_with_trimming,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ],
   "id": "8678947cec5d5a17",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:46:12.913154Z",
     "start_time": "2024-08-07T12:46:10.647115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_with_trimmed_history.invoke(\n",
    "    {\"input\": \"ä¿ºå«ä»€ä¹ˆ\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ],
   "id": "bd0ce5ae44e60005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"ä¿ºå«ä»€ä¹ˆ\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"ä¿ºå«ä»€ä¹ˆ\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"ä¿ºå«ä»€ä¹ˆ\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"ä¿ºå«ä»€ä¹ˆ\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:insert_history] [4ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableWithMessageHistoryInAsyncMode] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableWithMessageHistoryInAsyncMode] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence > chain:trim_messages] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence > chain:trim_messages] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history> > chain:RunnableParallel<chat_history>] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > chain:RunnableAssign<chat_history>] [4ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\\nHuman: ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ\\nAI: å¥½ç€å‘¢ï¼Ÿ\\nHuman: ä¿ºå«ä»€ä¹ˆ\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > llm:ChatOpenAI] [2.22s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œé‚£æˆ‘å°±çŸ¥é“æ‚¨å«ä»€ä¹ˆå•¦ã€‚\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œé‚£æˆ‘å°±çŸ¥é“æ‚¨å«ä»€ä¹ˆå•¦ã€‚\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 26,\n",
      "                \"prompt_tokens\": 62,\n",
      "                \"total_tokens\": 88\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7e386bf0-ba26-486e-92b1-622d8abfe329-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 62,\n",
      "              \"output_tokens\": 26,\n",
      "              \"total_tokens\": 88\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 26,\n",
      "      \"prompt_tokens\": 62,\n",
      "      \"total_tokens\": 88\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence] [2.22s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory > chain:RunnableBranch] [2.23s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"messages\",\n",
      "    \"AIMessage\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"content\": \"æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œé‚£æˆ‘å°±çŸ¥é“æ‚¨å«ä»€ä¹ˆå•¦ã€‚\",\n",
      "    \"response_metadata\": {\n",
      "      \"token_usage\": {\n",
      "        \"completion_tokens\": 26,\n",
      "        \"prompt_tokens\": 62,\n",
      "        \"total_tokens\": 88\n",
      "      },\n",
      "      \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "      \"system_fingerprint\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"id\": \"run-7e386bf0-ba26-486e-92b1-622d8abfe329-0\",\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 62,\n",
      "      \"output_tokens\": 26,\n",
      "      \"total_tokens\": 88\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  }\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableWithMessageHistory] [2.25s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œé‚£æˆ‘å°±çŸ¥é“æ‚¨å«ä»€ä¹ˆå•¦ã€‚', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 62, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7e386bf0-ba26-486e-92b1-622d8abfe329-0', usage_metadata={'input_tokens': 62, 'output_tokens': 26, 'total_tokens': 88})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T12:49:12.724310Z",
     "start_time": "2024-08-07T12:49:12.720678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# messagesæœ¬èº«ä¸ä¼šæœ‰åˆ«çš„ä¿®æ”¹æ“ä½œï¼Œåªæ˜¯ä¼ é€’ç»™æ¨¡å‹ä¹‹å‰åšäº†ä¸€äº›é¢„å¤„ç†ã€‚\n",
    "demo_ephemeral_chat_history.messages"
   ],
   "id": "1514ad0b6613a89d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='helloï¼Œæˆ‘æ˜¯Ethan'),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ'),\n",
       " AIMessage(content='å¥½ç€å‘¢ï¼Ÿ'),\n",
       " HumanMessage(content='ä¿ºå«ä»€ä¹ˆ'),\n",
       " AIMessage(content='æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œé‚£æˆ‘å°±çŸ¥é“æ‚¨å«ä»€ä¹ˆå•¦ã€‚', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 62, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7e386bf0-ba26-486e-92b1-622d8abfe329-0', usage_metadata={'input_tokens': 62, 'output_tokens': 26, 'total_tokens': 88})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ä»ä¸Šé¢çš„æ‰§è¡Œæ—¥å¿—ä¸Šå¯ä»¥çœ‹åˆ°ï¼Œ`trim_messages`å·²ç»å¸®æˆ‘ä»¬åšäº†å¯¹messageåšäº†å¤„ç†\n",
    "`trim_messages`ä¸­æ³¨é‡Šä¸­å†™äº†è¯¦ç»†çš„ç”¨æ³•ã€‚"
   ],
   "id": "7b9fab86d563fe64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Summary memory\n",
    "ä½¿ç”¨å¤§æ¨¡å‹ï¼Œå¯¹å¯¹è¯å†å²ä½œæ‘˜è¦ã€‚å°†æ‘˜è¦å½“ä½œå¯¹è¯ï¼Œå†å²å‘é€ç»™æ¨¡å‹ã€‚"
   ],
   "id": "9b0f580b82ddd181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:05:46.337603Z",
     "start_time": "2024-08-07T13:05:46.331664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "demo_ephemeral_chat_history.add_user_message(\"helloï¼Œæˆ‘æ˜¯Ethan\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"Hello!\")\n",
    "demo_ephemeral_chat_history.add_user_message(\"ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ\")\n",
    "demo_ephemeral_chat_history.add_ai_message(\"å¥½ç€å‘¢ï¼Ÿ\")\n",
    "\n",
    "demo_ephemeral_chat_history.messages"
   ],
   "id": "5c0cfc2cb9688d5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='helloï¼Œæˆ‘æ˜¯Ethan'),\n",
       " AIMessage(content='Hello!'),\n",
       " HumanMessage(content='ä»Šå„¿æ€ä¹ˆæ ·ï¼Ÿ'),\n",
       " AIMessage(content='å¥½ç€å‘¢ï¼Ÿ')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚å°½åŠ›å›ç­”æ‰€æœ‰é—®é¢˜ã€‚æä¾›çš„èŠå¤©è®°å½•åŒ…å«ä¸æ‚¨äº¤è°ˆçš„ç”¨æˆ·ç›¸å…³çš„äº‹å®ã€‚\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: demo_ephemeral_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ],
   "id": "174a785dda876b38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:05:56.423671Z",
     "start_time": "2024-08-07T13:05:56.420241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def summarize_messages(chain_input):\n",
    "    stored_messages = demo_ephemeral_chat_history.messages\n",
    "    if len(stored_messages) == 0:\n",
    "        return False\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"å°†ä¸Šè¿°èŠå¤©æ¶ˆæ¯æç‚¼æˆä¸€æ¡æ€»ç»“ä¿¡æ¯ã€‚å°½å¯èƒ½åŒ…å«è¯¦ç»†çš„å…·ä½“å†…å®¹ã€‚\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = summarization_prompt | chat\n",
    "\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
    "\n",
    "    demo_ephemeral_chat_history.clear()\n",
    "\n",
    "    demo_ephemeral_chat_history.add_message(summary_message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_summarization = (\n",
    "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
    "    | chain_with_message_history\n",
    ")"
   ],
   "id": "f80e749d719fdd22",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:06:52.526269Z",
     "start_time": "2024-08-07T13:06:46.532410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain_with_summarization.invoke(\n",
    "    {\"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ],
   "id": "6f04874bad808edd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"AI: Ethanä¸å¯¹æ–¹è¿›è¡Œç®€çŸ­çš„é—®å€™äº¤æµï¼Œè¯¢é—®å¯¹æ–¹çš„æƒ…å†µï¼Œè¡¨æ˜è‡ªå·±çŠ¶æ€è‰¯å¥½ã€‚\\nHuman: ä¿ºå«ä»€ä¹ˆåå­—\\nAI: æŠ±æ­‰ï¼Œæˆ‘æ— æ³•çŸ¥é“æ‚¨çš„çœŸå®å§“åã€‚æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨æƒ³è®©æˆ‘ç§°å‘¼æ‚¨ä»€ä¹ˆåå­—å—ï¼Ÿ\\nHuman: å°†ä¸Šè¿°èŠå¤©æ¶ˆæ¯æç‚¼æˆä¸€æ¡æ€»ç»“ä¿¡æ¯ã€‚å°½å¯èƒ½åŒ…å«è¯¦ç»†çš„å…·ä½“å†…å®¹ã€‚\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence > llm:ChatOpenAI] [2.54s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Ethanä¸å¯¹æ–¹è¿›è¡Œç®€çŸ­çš„é—®å€™äº¤æµï¼Œè¯¢é—®å¯¹æ–¹çš„æƒ…å†µï¼Œè¡¨æ˜è‡ªå·±çŠ¶æ€è‰¯å¥½ã€‚å¯¹æ–¹é—®\\\"ä¿ºå«ä»€ä¹ˆåå­—\\\"ï¼ŒEthanå›å¤è¯´æ— æ³•çŸ¥é“å¯¹æ–¹çš„çœŸå®å§“åï¼Œè¯¢é—®å¯¹æ–¹æ˜¯å¦å‘Šè¯‰ä»–æƒ³è¦è¢«ç§°å‘¼çš„åå­—ã€‚\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Ethanä¸å¯¹æ–¹è¿›è¡Œç®€çŸ­çš„é—®å€™äº¤æµï¼Œè¯¢é—®å¯¹æ–¹çš„æƒ…å†µï¼Œè¡¨æ˜è‡ªå·±çŠ¶æ€è‰¯å¥½ã€‚å¯¹æ–¹é—®\\\"ä¿ºå«ä»€ä¹ˆåå­—\\\"ï¼ŒEthanå›å¤è¯´æ— æ³•çŸ¥é“å¯¹æ–¹çš„çœŸå®å§“åï¼Œè¯¢é—®å¯¹æ–¹æ˜¯å¦å‘Šè¯‰ä»–æƒ³è¦è¢«ç§°å‘¼çš„åå­—ã€‚\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 88,\n",
      "                \"prompt_tokens\": 135,\n",
      "                \"total_tokens\": 223\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5e21357c-e67b-4e1b-920b-f132b6fae6fd-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 135,\n",
      "              \"output_tokens\": 88,\n",
      "              \"total_tokens\": 223\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 88,\n",
      "      \"prompt_tokens\": 135,\n",
      "      \"total_tokens\": 223\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages > chain:RunnableSequence] [2.54s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized> > chain:summarize_messages] [2.54s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": true\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized> > chain:RunnableParallel<messages_summarized>] [2.54s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<messages_summarized>] [2.55s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\",\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\",\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\",\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\",\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\",\n",
      "  \"messages_summarized\": true\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history> > chain:load_history] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history > chain:RunnableParallel<chat_history>] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:insert_history] [9ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableWithMessageHistoryInAsyncMode] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableWithMessageHistoryInAsyncMode] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: ä½ æ˜¯ä¸€ä¸ªAIå°åŠ©æ‰‹ï¼Œä½ è¦å°½å¯èƒ½çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜\\nAI: Ethanä¸å¯¹æ–¹è¿›è¡Œç®€çŸ­çš„é—®å€™äº¤æµï¼Œè¯¢é—®å¯¹æ–¹çš„æƒ…å†µï¼Œè¡¨æ˜è‡ªå·±çŠ¶æ€è‰¯å¥½ã€‚å¯¹æ–¹é—®\\\"ä¿ºå«ä»€ä¹ˆåå­—\\\"ï¼ŒEthanå›å¤è¯´æ— æ³•çŸ¥é“å¯¹æ–¹çš„çœŸå®å§“åï¼Œè¯¢é—®å¯¹æ–¹æ˜¯å¦å‘Šè¯‰ä»–æƒ³è¦è¢«ç§°å‘¼çš„åå­—ã€‚\\nHuman: åˆšæ‰æˆ‘è¯´æˆ‘å«ä»€ä¹ˆå‘€?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence > llm:ChatOpenAI] [3.21s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•ä¿å­˜ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯æˆ–å¯¹è¯å†å²ã€‚è¯·é—®æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨æƒ³è¢«ç§°å‘¼çš„åå­—å—ï¼Ÿæˆ‘ä¼šå°½é‡å¸®åŠ©æ‚¨çš„ã€‚\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•ä¿å­˜ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯æˆ–å¯¹è¯å†å²ã€‚è¯·é—®æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨æƒ³è¢«ç§°å‘¼çš„åå­—å—ï¼Ÿæˆ‘ä¼šå°½é‡å¸®åŠ©æ‚¨çš„ã€‚\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 54,\n",
      "                \"prompt_tokens\": 139,\n",
      "                \"total_tokens\": 193\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": \"fp_811936bd4f\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4eb2840d-9ad8-46ff-b1d2-05ef59bdea1b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 139,\n",
      "              \"output_tokens\": 54,\n",
      "              \"total_tokens\": 193\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 54,\n",
      "      \"prompt_tokens\": 139,\n",
      "      \"total_tokens\": 193\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": \"fp_811936bd4f\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch > chain:RunnableSequence] [3.21s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory > chain:RunnableBranch] [3.37s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"messages\",\n",
      "    \"AIMessage\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"content\": \"å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•ä¿å­˜ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯æˆ–å¯¹è¯å†å²ã€‚è¯·é—®æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨æƒ³è¢«ç§°å‘¼çš„åå­—å—ï¼Ÿæˆ‘ä¼šå°½é‡å¸®åŠ©æ‚¨çš„ã€‚\",\n",
      "    \"response_metadata\": {\n",
      "      \"token_usage\": {\n",
      "        \"completion_tokens\": 54,\n",
      "        \"prompt_tokens\": 139,\n",
      "        \"total_tokens\": 193\n",
      "      },\n",
      "      \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "      \"system_fingerprint\": \"fp_811936bd4f\",\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"id\": \"run-4eb2840d-9ad8-46ff-b1d2-05ef59bdea1b-0\",\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 139,\n",
      "      \"output_tokens\": 54,\n",
      "      \"total_tokens\": 193\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": []\n",
      "  }\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithMessageHistory] [3.39s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [5.97s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•ä¿å­˜ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯æˆ–å¯¹è¯å†å²ã€‚è¯·é—®æ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨æƒ³è¢«ç§°å‘¼çš„åå­—å—ï¼Ÿæˆ‘ä¼šå°½é‡å¸®åŠ©æ‚¨çš„ã€‚', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 139, 'total_tokens': 193}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_811936bd4f', 'finish_reason': 'stop', 'logprobs': None}, id='run-4eb2840d-9ad8-46ff-b1d2-05ef59bdea1b-0', usage_metadata={'input_tokens': 139, 'output_tokens': 54, 'total_tokens': 193})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "è™½ç„¶ä»–æ²¡æœ‰ç­”å‡ºæ¥ï¼Œä½†æ˜¯ä»debugçš„æ—¥å¿—é‡Œé¢å¯ä»¥çœ‹åˆ°åšäº†æ¶ˆæ¯çš„æ‘˜è¦ã€‚\n",
   "id": "2bafd0576b0c34bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## å¦‚ä½•åšæ£€ç´¢ï¼Ÿ\n",
    "\n",
    "æ£€ç´¢æ˜¯èŠå¤©æœºå™¨äººçš„ä¸€ä¸ªå¾ˆæœ‰ç”¨çš„ç‰¹ç‚¹ã€‚é€šè¿‡æ£€ç´¢ï¼Œå¯ä»¥è®©æ¨¡å‹è·å–æœ€æ–°çš„æ¶ˆæ¯ã€‚å›ç­”çŸ¥è¯†ã€‚åˆ©ç”¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå›ç­”çŸ¥è¯†ã€‚\n",
    "\n",
    "### Setup"
   ],
   "id": "9d71298bebd17c82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:18:42.238764Z",
     "start_time": "2024-08-07T13:18:42.208417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)"
   ],
   "id": "f84fb0fe874d2014",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åˆ›å»ºæ£€ç´¢å™¨",
   "id": "e0526d1ffc918311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:19:08.120761Z",
     "start_time": "2024-08-07T13:19:04.824521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ç½‘é¡µåŠ è½½ \n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "data = loader.load()\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# å‘é‡åŒ–\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "# k is the number of chunks to retrieve\n",
    "retriever = vectorstore.as_retriever(k=4)\n",
    "\n",
    "docs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")\n",
    "\n",
    "docs"
   ],
   "id": "4e60bb9251321587",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Document chains\n",
    "è¿™é‡Œçš„æ­¥éª¤å’Œä¹‹å‰å¾ˆç›¸ä¼¼äº†ã€‚åˆ›å»ºä¸€ä¸ªæ–‡æ¡£çš„æ£€ç´¢é“¾ã€‚ä½¿ç”¨`create_stuff_documents_chain`æ–¹æ³•,å°†æ‰€æœ‰çš„æ–‡æ¡£å¥—å…¥åˆ°prompt"
   ],
   "id": "21691b034966cd81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:22:08.922258Z",
     "start_time": "2024-08-07T13:22:08.894092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "Answer the user's questions based on the below context. \n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_TEMPLATE,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
   ],
   "id": "e56d6026a702437c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:22:11.932231Z",
     "start_time": "2024-08-07T13:22:09.820027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"context\": docs,\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ],
   "id": "f7a0f22426194a60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > chain:format_inputs] [5ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: \\nAnswer the user's questions based on the below context. \\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \\\"I don't know\\\":\\n\\n<context>\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n</context>\\n\\nHuman: Can LangSmith help test my LLM applications?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > llm:ChatOpenAI] [2.08s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, LangSmith allows you to closely monitor and evaluate your LLM applications, which can help in testing them effectively.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, LangSmith allows you to closely monitor and evaluate your LLM applications, which can help in testing them effectively.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 24,\n",
      "                \"prompt_tokens\": 304,\n",
      "                \"total_tokens\": 328\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_276aa25277\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-bc3449f0-d349-4850-900d-83c4b06e964a-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 304,\n",
      "              \"output_tokens\": 24,\n",
      "              \"total_tokens\": 328\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 24,\n",
      "      \"prompt_tokens\": 304,\n",
      "      \"total_tokens\": 328\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_276aa25277\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Yes, LangSmith allows you to closely monitor and evaluate your LLM applications, which can help in testing them effectively.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:stuff_documents_chain] [2.10s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Yes, LangSmith allows you to closely monitor and evaluate your LLM applications, which can help in testing them effectively.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, LangSmith allows you to closely monitor and evaluate your LLM applications, which can help in testing them effectively.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieval chains\n",
    "\n",
    "å°†document chainå’Œ Retrieval chainç»“åˆèµ·æ¥"
   ],
   "id": "13e2c83bb30840ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:16.197477Z",
     "start_time": "2024-08-07T13:31:16.194075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def parse_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "\n",
    "# ä¸‹é¢çš„æ„æ€æ˜¯è¯´ï¼Œå…ˆè·å–ç”¨æˆ·è¾“å…¥messages listçš„æœ€åä¸€ä¸ªæ¶ˆæ¯ï¼Œå¥—å…¥åˆ°æ£€ç´¢å™¨ä¸­\n",
    "# åšæ£€ç´¢ï¼Œä¹‹åï¼Œå°†æ£€ç´¢åˆ°çš„ç­”æ¡ˆä»£å…¥åˆ°retrieve chainå»åš\n",
    "retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=parse_retriever_input | retriever, # è¿™æ˜¯æ£€ç´¢çš„channelï¼Œ\n",
    ").assign(\n",
    "    answer=document_chain, # è¿™æ˜¯æ–‡æ¡£çš„chainï¼Œ\n",
    ")"
   ],
   "id": "995d434c6f1196ac",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:21.359269Z",
     "start_time": "2024-08-07T13:31:17.163650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ],
   "id": "f354d303d24189c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableSequence > chain:parse_retriever_input] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableSequence > chain:parse_retriever_input] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Can LangSmith help test my LLM applications?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:RunnableSequence] [1.36s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [1.36s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] [1.36s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] [4ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: \\nAnswer the user's questions based on the below context. \\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \\\"I don't know\\\":\\n\\n<context>\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n</context>\\n\\nHuman: Can LangSmith help test my LLM applications?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] [2.76s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 39,\n",
      "                \"prompt_tokens\": 310,\n",
      "                \"total_tokens\": 349\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_811936bd4f\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-71cc28a0-5c22-4d1e-8a4d-bd82908df76b-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 310,\n",
      "              \"output_tokens\": 39,\n",
      "              \"total_tokens\": 349\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 39,\n",
      "      \"prompt_tokens\": 310,\n",
      "      \"total_tokens\": 349\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_811936bd4f\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] [2.77s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] [2.78s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"answer\": \"LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] [2.79s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [4.17s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
       " 'context': [Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],\n",
       " 'answer': 'LangSmith is a platform for building production-grade LLM applications and allows you to closely monitor and evaluate your application. While it focuses on monitoring and evaluation, it does not specifically mention testing capabilities.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æŸ¥è¯¢è½¬æ¢\n",
    "\n",
    "ä½¿ç”¨æ£€ç´¢å™¨åšæ£€ç´¢çš„æ—¶å€™ï¼Œæ£€ç´¢æœ¬èº«æ˜¯æ²¡æœ‰ä¸Šä¸‹æ–‡çš„ã€‚å¦‚æœæ£€ç´¢çš„é—®é¢˜æ‰¾ä¸åˆ°æ£€ç´¢å™¨ä¼šè¿”å›ä¸€äº›æ— å…³çš„æ–‡æ¡£ã€‚æ¯”å¦‚ä½ åœ¨é—®ï¼ŸLang smithçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿæ£€ç´¢å™¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç›¸å…³çš„æ–‡æ¡£ï¼Œä½†æ˜¯ï¼Œå¦‚æœä½ åŸºäºå¯¹è¯çš„ä¸Šä¸‹æ–‡å„¿ï¼Œä½ å†é—®ã€‚è¯¦ç»†è§£é‡Šä¸‹ã€‚è§£é”å™¨æ˜¯æŸ¥ä¸åˆ°ä»»ä½•å’ŒLang Smithä½œç”¨æœ‰å…³çš„æ–‡æ¡£çš„ï¼Œç„¶åè¿”å›äº†ä¸€äº›æ— å…³çš„æ–‡æ¡£ã€‚\n",
    "è¿™ä¸ªæ—¶å€™å°±éœ€è¦æŸ¥è¯¢è½¬æ¢ã€‚"
   ],
   "id": "be085bc4238a9cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:35:43.455957Z",
     "start_time": "2024-08-07T13:35:42.154685Z"
    }
   },
   "cell_type": "code",
   "source": "retriever.invoke(\"Tell me more!\") # è¿”å›äº†ä¸€äº›æ— å…³çš„æ–‡æ¡£",
   "id": "852f90ed51ad3f96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥å°†å¯¹è¯é€šè¿‡å¤§æ¨¡å‹æ¥ç”ŸæˆæŸ¥è¯¢ã€‚",
   "id": "3926220648093b5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:37:47.560837Z",
     "start_time": "2024-08-07T13:37:45.455362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_transformation_chain = query_transform_prompt | chat\n",
    "\n",
    "query_transformation_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "            AIMessage(\n",
    "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
    "            ),\n",
    "            HumanMessage(content=\"Tell me more!\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ],
   "id": "be1cb0d0e8672394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Can LangSmith help test my LLM applications?\\nAI: Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\\nHuman: Tell me more!\\nHuman: Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.09s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\"LangSmith LLM application testing and evaluation\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\\\"LangSmith LLM application testing and evaluation\\\"\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 145,\n",
      "                \"total_tokens\": 155\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_5aa43294a1\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0c88e751-ac9d-4a49-984d-fa0061bde854-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 145,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 155\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 145,\n",
      "      \"total_tokens\": 155\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [2.10s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"LangSmith LLM application testing and evaluation\"', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 145, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_5aa43294a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-0c88e751-ac9d-4a49-984d-fa0061bde854-0', usage_metadata={'input_tokens': 145, 'output_tokens': 10, 'total_tokens': 155})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OKï¼Œå¯ä»¥çœ‹åˆ°ã€‚å·²ç»ç”Ÿæˆäº†æˆ‘ä»¬æƒ³è¦çš„æŸ¥è¯¢è¯­å¥ã€‚ä¸‹é¢å°†ä»–å’Œä¹‹å‰çš„chainç»“åˆèµ·æ¥ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œ`RunnableBranch`è¡¨ç¤ºçš„æ˜¯ä¸€ä¸ªåˆ†æ”¯æ¡ä»¶,ä»–è¦æ±‚ä¼ å…¥ä¸€äº›åˆ¤æ–­æ¡ä»¶å’Œä¸€ä¸ªdefaultçš„æ¡ä»¶ï¼Œåœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå¦‚æœmessageåªæœ‰ä¸€æ¡ï¼Œç›´æ¥å»åšæ£€ç´¢ã€‚å¦åˆ™å°±è¦ç»è¿‡ä¸Šé¢çš„æŸ¥è¯¢è½¬æ¢ã€‚å†å»åšæ£€ç´¢ã€‚"
   ],
   "id": "5a9ccb4146aa3371"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:41:39.566348Z",
     "start_time": "2024-08-07T13:41:39.562377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "query_transforming_retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
    "        # If only one message, then we just pass that message's content to retriever\n",
    "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
    "    ),\n",
    "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
    "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
    ").with_config(run_name=\"chat_retriever_chain\")"
   ],
   "id": "9d591b4935dd2c4a",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:41:40.446411Z",
     "start_time": "2024-08-07T13:41:40.442651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "Answer the user's questions based on the below context. \n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_TEMPLATE,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
    "\n",
    "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=query_transforming_retriever_chain,\n",
    ").assign(\n",
    "    answer=document_chain,\n",
    ")"
   ],
   "id": "6a7c829c6fe2a638",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:41:51.221201Z",
     "start_time": "2024-08-07T13:41:47.560352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "        ]\n",
    "    }\n",
    ")æ²¡äº‹"
   ],
   "id": "2211b7fb0b924806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": true\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Can LangSmith help test my LLM applications?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence] [1.34s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain] [1.34s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [1.34s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] [1.35s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: \\nAnswer the user's questions based on the below context. \\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \\\"I don't know\\\":\\n\\n<context>\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\nGet started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n</context>\\n\\nHuman: Can LangSmith help test my LLM applications?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] [2.27s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 51,\n",
      "                \"prompt_tokens\": 310,\n",
      "                \"total_tokens\": 361\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_5aa43294a1\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e840aa19-a654-4b0b-a929-fac8dd0054ef-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 310,\n",
      "              \"output_tokens\": 51,\n",
      "              \"total_tokens\": 361\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 51,\n",
      "      \"prompt_tokens\": 310,\n",
      "      \"total_tokens\": 361\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] [2.28s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] [2.28s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"answer\": \"Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] [2.29s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.65s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
       " 'context': [Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],\n",
       " 'answer': 'Yes, LangSmith is designed to help you closely monitor and evaluate your LLM applications, so you can ship quickly and with confidence. It is a platform for building production-grade LLM applications, and it allows you to test and monitor your applications effectively.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ä¸Šé¢åªæœ‰ä¸€æ¡messageï¼Œæ‰€ä»¥ä»–ä¸ä¼šå»åšæŸ¥è¯¢è½¬æ¢ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘ä¼šè¾“å…¥å¤šä¸ªmessageã€‚"
   ],
   "id": "3c6a17313750664e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:43:12.107243Z",
     "start_time": "2024-08-07T13:43:03.923910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_retrieval_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "            AIMessage(\n",
    "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
    "            ),\n",
    "            HumanMessage(content=\"Tell me more!\"),\n",
    "        ],\n",
    "    }\n",
    ")å•ªå•ªç„¶åæˆ‘æƒ³å•Šï¼Ÿ"
   ],
   "id": "d37d9d1634120f7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Can LangSmith help test my LLM applications?\\nAI: Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\\nHuman: Tell me more!\\nHuman: Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > llm:ChatOpenAI] [1.97s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\\"LangSmith LLM application testing and evaluation\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\\\"LangSmith LLM application testing and evaluation\\\"\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 10,\n",
      "                \"prompt_tokens\": 145,\n",
      "                \"total_tokens\": 155\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_5aa43294a1\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-49011f25-4d82-4961-b266-0718cadcd8a3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 145,\n",
      "              \"output_tokens\": 10,\n",
      "              \"total_tokens\": 155\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 10,\n",
      "      \"prompt_tokens\": 145,\n",
      "      \"total_tokens\": 155\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence > parser:StrOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"\\\"LangSmith LLM application testing and evaluation\\\"\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain > chain:RunnableSequence] [3.92s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:chat_retriever_chain] [3.92s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"document\",\n",
      "        \"Document\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"page_content\": \"\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\",\n",
      "        \"metadata\": {\n",
      "          \"description\": \"LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!\",\n",
      "          \"language\": \"en\",\n",
      "          \"source\": \"https://docs.smith.langchain.com/overview\",\n",
      "          \"title\": \"Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\"\n",
      "        },\n",
      "        \"type\": \"Document\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [3.93s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<context>] [3.93s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context> > chain:format_docs] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs > chain:RunnableParallel<context>] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"context\": \"Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > chain:format_inputs] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: \\nAnswer the user's questions based on the below context. \\nIf the context doesn't contain any relevant information to the question, don't make something up and just say \\\"I don't know\\\":\\n\\n<context>\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmithâ€‹PythonTypeScriptpip install -U\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\\n\\n\\\"1.0.0\\\",      revision_id: \\\"beta\\\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.\\n</context>\\n\\nHuman: Can LangSmith help test my LLM applications?\\nAI: Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\\nHuman: Tell me more!\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > llm:ChatOpenAI] [4.20s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 244,\n",
      "                \"prompt_tokens\": 582,\n",
      "                \"total_tokens\": 826\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_276aa25277\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e3d13f45-9d2d-49c1-9b3f-8703cef97609-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 582,\n",
      "              \"output_tokens\": 244,\n",
      "              \"total_tokens\": 826\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 244,\n",
      "      \"prompt_tokens\": 582,\n",
      "      \"total_tokens\": 826\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "    \"system_fingerprint\": \"fp_276aa25277\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer> > chain:stuff_documents_chain] [4.21s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer> > chain:RunnableParallel<answer>] [4.22s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"answer\": \"LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableAssign<answer>] [4.22s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [8.17s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),\n",
       "  HumanMessage(content='Tell me more!')],\n",
       " 'context': [Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith\\u200bPythonTypeScriptpip install -U', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='\"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       "  Document(page_content='\"1.0.0\",      revision_id: \"beta\",    },  });Learn more about evaluation in the how-to guides.Was this page helpful?You can leave detailed feedback on GitHub.NextTutorials1. Install LangSmith2. Create an API key3. Set up your environment4. Log your first trace5. Run your first evaluationCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright Â© 2024 LangChain, Inc.', metadata={'description': 'LangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Get started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})],\n",
       " 'answer': 'LangSmith is designed for building production-grade LLM applications, providing tools to closely monitor and evaluate your application. Here are some key features:\\n\\n1. **Monitoring and Evaluation**: You can track the performance of your LLM applications, ensuring they meet your quality standards.\\n\\n2. **Logging Traces**: LangSmith allows you to log traces of your application, which can help in debugging and understanding how your model is performing in real-time.\\n\\n3. **Visualizing Data**: It provides visualizations for latency and token usage statistics, making it easier to identify bottlenecks or inefficiencies.\\n\\n4. **Fine-tuning**: You can expand your evaluation sets by editing examples and adding them to datasets, which can help in fine-tuning your model for better performance.\\n\\n5. **Ease of Use**: LangSmith works independently and does not require the use of LangChain, making it straightforward to integrate into your existing workflows.\\n\\n6. **Community Support**: There are resources available such as tutorials, how-to guides, and community support through platforms like Discord and GitHub.\\n\\nOverall, LangSmith aims to help developers ship their LLM applications quickly and with confidence by providing robust tools for testing and evaluation.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Okï¼Œå¾—åˆ°äº†æƒ³è¦çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "### Streaming\n",
    "åŸºäºLCELè¯­å¥çš„chainï¼Œå¯ä»¥è°ƒç”¨streamæ–¹æ³•æ¥æµæ°´è¿”å›"
   ],
   "id": "da629777f371f596"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T13:48:08.888925Z",
     "start_time": "2024-08-07T13:47:55.781628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_debug(False)\n",
    "stream = conversational_retrieval_chain.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "            AIMessage(\n",
    "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
    "            ),\n",
    "            HumanMessage(content=\"Tell me more!\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk)"
   ],
   "id": "bcd9eb7d49acba95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ''}\n",
      "{'answer': 'Lang'}\n",
      "{'answer': 'Smith'}\n",
      "{'answer': ' is'}\n",
      "{'answer': ' designed'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' building'}\n",
      "{'answer': ' production'}\n",
      "{'answer': '-grade'}\n",
      "{'answer': ' L'}\n",
      "{'answer': 'LM'}\n",
      "{'answer': ' applications'}\n",
      "{'answer': ','}\n",
      "{'answer': ' providing'}\n",
      "{'answer': ' tools'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' monitoring'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' evaluating'}\n",
      "{'answer': ' your'}\n",
      "{'answer': ' applications'}\n",
      "{'answer': ' effectively'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' Here'}\n",
      "{'answer': ' are'}\n",
      "{'answer': ' some'}\n",
      "{'answer': ' key'}\n",
      "{'answer': ' features'}\n",
      "{'answer': ':\\n\\n'}\n",
      "{'answer': '1'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' **'}\n",
      "{'answer': 'Testing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' Evaluation'}\n",
      "{'answer': '**'}\n",
      "{'answer': ':'}\n",
      "{'answer': ' You'}\n",
      "{'answer': ' can'}\n",
      "{'answer': ' closely'}\n",
      "{'answer': ' monitor'}\n",
      "{'answer': ' your'}\n",
      "{'answer': ' application'}\n",
      "{'answer': ','}\n",
      "{'answer': ' allowing'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' thorough'}\n",
      "{'answer': ' testing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' evaluation'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' its'}\n",
      "{'answer': ' performance'}\n",
      "{'answer': '.\\n\\n'}\n",
      "{'answer': '2'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' **'}\n",
      "{'answer': 'Editing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' Exp'}\n",
      "{'answer': 'anding'}\n",
      "{'answer': ' D'}\n",
      "{'answer': 'atasets'}\n",
      "{'answer': '**'}\n",
      "{'answer': ':'}\n",
      "{'answer': ' Lang'}\n",
      "{'answer': 'Smith'}\n",
      "{'answer': ' enables'}\n",
      "{'answer': ' you'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' quickly'}\n",
      "{'answer': ' edit'}\n",
      "{'answer': ' examples'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' add'}\n",
      "{'answer': ' them'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' datasets'}\n",
      "{'answer': ','}\n",
      "{'answer': ' which'}\n",
      "{'answer': ' helps'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' expanding'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' evaluation'}\n",
      "{'answer': ' sets'}\n",
      "{'answer': ' or'}\n",
      "{'answer': ' fine'}\n",
      "{'answer': '-t'}\n",
      "{'answer': 'uning'}\n",
      "{'answer': ' models'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' better'}\n",
      "{'answer': ' performance'}\n",
      "{'answer': '.\\n\\n'}\n",
      "{'answer': '3'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' **'}\n",
      "{'answer': 'Monitoring'}\n",
      "{'answer': '**'}\n",
      "{'answer': ':'}\n",
      "{'answer': ' It'}\n",
      "{'answer': ' offers'}\n",
      "{'answer': ' capabilities'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' log'}\n",
      "{'answer': ' all'}\n",
      "{'answer': ' traces'}\n",
      "{'answer': ','}\n",
      "{'answer': ' visualize'}\n",
      "{'answer': ' latency'}\n",
      "{'answer': ','}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' track'}\n",
      "{'answer': ' token'}\n",
      "{'answer': ' usage'}\n",
      "{'answer': ' statistics'}\n",
      "{'answer': ','}\n",
      "{'answer': ' which'}\n",
      "{'answer': ' aids'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' understanding'}\n",
      "{'answer': ' the'}\n",
      "{'answer': \" application's\"}\n",
      "{'answer': ' behavior'}\n",
      "{'answer': '.\\n\\n'}\n",
      "{'answer': '4'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' **'}\n",
      "{'answer': 'Trou'}\n",
      "{'answer': 'bles'}\n",
      "{'answer': 'hooting'}\n",
      "{'answer': '**'}\n",
      "{'answer': ':'}\n",
      "{'answer': ' You'}\n",
      "{'answer': ' can'}\n",
      "{'answer': ' troubleshoot'}\n",
      "{'answer': ' specific'}\n",
      "{'answer': ' issues'}\n",
      "{'answer': ' as'}\n",
      "{'answer': ' they'}\n",
      "{'answer': ' arise'}\n",
      "{'answer': ','}\n",
      "{'answer': ' making'}\n",
      "{'answer': ' it'}\n",
      "{'answer': ' easier'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' maintain'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' improve'}\n",
      "{'answer': ' your'}\n",
      "{'answer': ' application'}\n",
      "{'answer': '.\\n\\n'}\n",
      "{'answer': '5'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' **'}\n",
      "{'answer': 'No'}\n",
      "{'answer': ' Need'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' Lang'}\n",
      "{'answer': 'Chain'}\n",
      "{'answer': '**'}\n",
      "{'answer': ':'}\n",
      "{'answer': ' Lang'}\n",
      "{'answer': 'Smith'}\n",
      "{'answer': ' operates'}\n",
      "{'answer': ' independently'}\n",
      "{'answer': ','}\n",
      "{'answer': ' so'}\n",
      "{'answer': ' you'}\n",
      "{'answer': \" don't\"}\n",
      "{'answer': ' need'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' use'}\n",
      "{'answer': ' Lang'}\n",
      "{'answer': 'Chain'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' benefit'}\n",
      "{'answer': ' from'}\n",
      "{'answer': ' its'}\n",
      "{'answer': ' features'}\n",
      "{'answer': '.\\n\\n'}\n",
      "{'answer': 'These'}\n",
      "{'answer': ' functionalities'}\n",
      "{'answer': ' make'}\n",
      "{'answer': ' Lang'}\n",
      "{'answer': 'Smith'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' valuable'}\n",
      "{'answer': ' tool'}\n",
      "{'answer': ' for'}\n",
      "{'answer': ' developers'}\n",
      "{'answer': ' looking'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' enhance'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' quality'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' reliability'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' their'}\n",
      "{'answer': ' L'}\n",
      "{'answer': 'LM'}\n",
      "{'answer': ' applications'}\n",
      "{'answer': '.'}\n",
      "{'answer': ''}\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## å¦‚ä½•ä½¿ç”¨å·¥å…·\n",
    "\n",
    "ä¹‹å‰å·²ç»è¯´è¿‡æ¨¡å‹æ˜¯å¦‚ä½•è°ƒç”¨å·¥å…·çš„èŠ‚ã€‚åœ¨è¿™é‡Œå°†ä¼šæ¼”ç¤ºå¦‚ä½•é€šè¿‡agentæ¥è°ƒç”¨å·¥å…·ã€‚\n",
    "### åˆ›å»ºagent\n",
    "æˆ‘ä»¬ä¼šä½¿ç”¨åˆ°`AgentExecutor, create_tool_calling_agent`"
   ],
   "id": "f610ebf9ba096575"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T14:08:54.891264Z",
     "start_time": "2024-08-07T14:08:54.880386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def current_date():\n",
    "    \"\"\"è¿”å›å½“å‰æ—¶é—´ï¼Œåªæœ‰å½“ç”¨æˆ·è¯¢é—®å½“å‰çš„æ—¶å€™å¯ä»¥ä½¿ç”¨æ­¤å·¥å…·\"\"\"\n",
    "    return \"2024-08-04 12:12:12\"\n",
    "\n",
    "tools = [current_date]\n",
    "# Adapted from https://smith.langchain.com/hub/jacob/tool-calling-agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(chat, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ],
   "id": "70e491b83f512d58",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T14:10:02.426463Z",
     "start_time": "2024-08-07T14:10:00.259273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "agent_executor.invoke({\"messages\": [HumanMessage(content=\"I'm Nemo!\")]})"
   ],
   "id": "547f710fdff5df40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mHi Nemo! How can I assist you today?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"I'm Nemo!\")],\n",
       " 'output': 'Hi Nemo! How can I assist you today?'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T14:12:56.950219Z",
     "start_time": "2024-08-07T14:12:53.108934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_debug(True)\n",
    "agent_executor.invoke({\"messages\": [HumanMessage(content=\"ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ\")]})"
   ],
   "id": "8fee8dd198d14a68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [6ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\\nHuman: ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [2.53s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "          \"system_fingerprint\": \"fp_811936bd4f\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"index\": 0,\n",
      "                  \"id\": \"call_gtgakhiDWQkzRz9mnxaUIGXI\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{}\",\n",
      "                    \"name\": \"current_date\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"finish_reason\": \"tool_calls\",\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_811936bd4f\"\n",
      "            },\n",
      "            \"type\": \"AIMessageChunk\",\n",
      "            \"id\": \"run-7404c2e3-35a1-416f-949a-403076d38ae9\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"current_date\",\n",
      "                \"args\": {},\n",
      "                \"id\": \"call_gtgakhiDWQkzRz9mnxaUIGXI\"\n",
      "              }\n",
      "            ],\n",
      "            \"tool_call_chunks\": [\n",
      "              {\n",
      "                \"name\": \"current_date\",\n",
      "                \"args\": \"{}\",\n",
      "                \"id\": \"call_gtgakhiDWQkzRz9mnxaUIGXI\",\n",
      "                \"index\": 0\n",
      "              }\n",
      "            ],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] [2.55s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[tool/start]\u001B[0m \u001B[1m[chain:AgentExecutor > tool:current_date] Entering Tool run with input:\n",
      "\u001B[0m\"{}\"\n",
      "\u001B[36;1m\u001B[1;3m[tool/end]\u001B[0m \u001B[1m[chain:AgentExecutor > tool:current_date] [0ms] Exiting Tool run with output:\n",
      "\u001B[0m\"2024-08-04 12:12:12\"\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [4ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [6ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\\nHuman: ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ\\nAI: \\nTool: 2024-08-04 12:12:12\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [1.26s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ç°åœ¨æ˜¯2024å¹´8æœˆ4æ—¥ï¼Œ12ç‚¹12åˆ†12ç§’ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "          \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ç°åœ¨æ˜¯2024å¹´8æœˆ4æ—¥ï¼Œ12ç‚¹12åˆ†12ç§’ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ\",\n",
      "            \"response_metadata\": {\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "            },\n",
      "            \"type\": \"AIMessageChunk\",\n",
      "            \"id\": \"run-f29072f1-b54a-4195-8764-2772d45848e7\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] [1.27s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor] [3.83s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"ç°åœ¨æ˜¯2024å¹´8æœˆ4æ—¥ï¼Œ12ç‚¹12åˆ†12ç§’ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ')],\n",
       " 'output': 'ç°åœ¨æ˜¯2024å¹´8æœˆ4æ—¥ï¼Œ12ç‚¹12åˆ†12ç§’ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªagentå·²ç»è°ƒç”¨äº†å·¥å…·ï¼Œå¹¶ä¸”ä¹Ÿå¯ä»¥æ­£å¸¸çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "### Conversational responses\n",
    "å› ä¸ºæˆ‘ä»¬çš„æç¤ºä¸­åŒ…å«äº†èŠå¤©è®°å½•æ¶ˆæ¯çš„å ä½ç¬¦ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯ä»¥è€ƒè™‘ä¹‹å‰çš„äº’åŠ¨ï¼Œå¹¶åƒæ ‡å‡†èŠå¤©æœºå™¨äººä¸€æ ·è¿›è¡Œå¯¹è¯å›åº”ï¼š"
   ],
   "id": "6af290ad7d82f6d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T14:17:40.219643Z",
     "start_time": "2024-08-07T14:17:38.445235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"I'm Nemo!\"),\n",
    "            AIMessage(content=\"Hello Nemo! How can I assist you today?\"),\n",
    "            HumanMessage(content=\"What is my name?\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ],
   "id": "a1deeac191f94f7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [3ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [6ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\\nHuman: I'm Nemo!\\nAI: Hello Nemo! How can I assist you today?\\nHuman: What is my name?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [1.74s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Your name is Nemo!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "          \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Your name is Nemo!\",\n",
      "            \"response_metadata\": {\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"model_name\": \"gpt-3.5-turbo-1106\",\n",
      "              \"system_fingerprint\": \"fp_5aa43294a1\"\n",
      "            },\n",
      "            \"type\": \"AIMessageChunk\",\n",
      "            \"id\": \"run-c76c1165-73e8-4148-b80c-0e545cf59b4e\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor > chain:RunnableSequence] [1.76s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:AgentExecutor] [1.76s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"Your name is Nemo!\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"I'm Nemo!\"),\n",
       "  AIMessage(content='Hello Nemo! How can I assist you today?'),\n",
       "  HumanMessage(content='What is my name?')],\n",
       " 'output': 'Your name is Nemo!'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "å°±å¯ä»¥ç»“åˆHistoryæ¥åŒ…è£…agentï¼Œè®©ä»–å¯ä»¥è®°ä½å¯¹è¯å†å²",
   "id": "ca21048aca1c1a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T14:23:03.023074Z",
     "start_time": "2024-08-07T14:23:00.541053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_debug(False)\n",
    "agent = create_tool_calling_agent(chat, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "demo_ephemeral_chat_history_for_chain = ChatMessageHistory()\n",
    "conversational_agent_executor = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: demo_ephemeral_chat_history_for_chain,\n",
    "    input_messages_key=\"messages\",\n",
    "    output_messages_key=\"output\",\n",
    ")\n",
    "\n",
    "conversational_agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(\"I'm Nemo!\")]},\n",
    "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
    ")"
   ],
   "id": "297576355e0c8f54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mHi Nemo! How can I assist you today?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"I'm Nemo!\")],\n",
       " 'output': 'Hi Nemo! How can I assist you today?'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
