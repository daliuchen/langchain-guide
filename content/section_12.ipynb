{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RAG 一些相关问题\n",
    "\n",
    "\n",
    "## RAG返回source\n",
    "返回本次回答引用了哪些文档。\n",
    "\n",
    "`通过create_retrieval_chain`来做，代码如下"
   ],
   "id": "3094ebb661febfb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T15:36:55.751601Z",
     "start_time": "2024-07-30T15:36:47.157839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://guangzhengli.com/blog/zh/vector-database/\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"你是一个ai助手，回答用户的问题\"\n",
    "    \"使用下面的上下文来回答\"\n",
    "    \"如果你不知道，要回答我不知道\"\n",
    "    \"使用简短精炼的语句表单，最多三句话\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\" # 这里content的位置就是从向量数据库中找到数据之后，塞到Prompt的位置，\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ],
   "id": "f141da42984a440c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:03:13.379348Z",
     "start_time": "2024-07-29T15:03:13.371489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(rag_chain.get_prompts()[0].pretty_print())\n",
    "print(rag_chain.get_prompts()[1].pretty_print())"
   ],
   "id": "b48707afae6c48f1",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:44:54.825684Z",
     "start_time": "2024-07-29T15:44:52.665411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = rag_chain.invoke({\"input\": \"向量数据库的优点是什么？\"})\n",
    "print(result)"
   ],
   "id": "881637120737ace6",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:45:01.633312Z",
     "start_time": "2024-07-29T15:45:01.629348Z"
    }
   },
   "cell_type": "code",
   "source": "result[\"context\"][0]",
   "id": "35066abfdc024385",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如上所述。在返回值中，会返回完整的引用的document的切片",
   "id": "a8960d25f79acf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 返回引用\n",
    "官网文档：\n",
    "https://python.langchain.com/v0.2/docs/how_to/qa_citations/\n",
    "\n",
    "在这里我演示前面两种方式\n",
    "1. 使用工具调用来引用文档 ID；\n",
    "2. 使用工具调用来引用文档 ID 并提供文本片段；"
   ],
   "id": "85d13d0fb8496080"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:46:42.216637Z",
     "start_time": "2024-07-29T15:46:42.209817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在上面的例子中已经返回了这次回答所涉及的所有的文档内容，都是放在`context`中，在下面的两个例子中，会以这个例子为基础来做。\n",
    "result[\"context\"][0]"
   ],
   "id": "c736800e0fde4cbf",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 使用工具调用来引用文档\n",
    "\n",
    "原理：首先先给文档编号，指定文档id，通过模型支持的函数调用来实现,`with_structured_output`"
   ],
   "id": "3a39e9a0efec7ee4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:56:24.975130Z",
     "start_time": "2024-07-29T15:56:24.967132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# 规定模型调用的工具，之前说过，对于`with_structured_output`的实现也是通过`bind_tools`来实现的\n",
    "\n",
    "class CitedAnswer(BaseModel):\n",
    "    \"\"\"根据提供的来源回答用户问题，并引用所使用的来源。\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"根据提供的来源，回答用户的问题。\",\n",
    "    )\n",
    "    citations: List[int] = Field(\n",
    "        ...,\n",
    "        description=\"回答这个问题所引用的文档id\",\n",
    "    )\n",
    "    "
   ],
   "id": "33f8d8726e06165c",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "下面是一个简单的原理说明demo，之后会将找到的document格式化为下面的样子，代入到promot中处理",
   "id": "118d577cd5b20872"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:57:20.644140Z",
     "start_time": "2024-07-29T15:57:18.780545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(CitedAnswer)\n",
    "\n",
    "example_q = \"\"\"What Brian's height?\n",
    "\n",
    "Source: 1\n",
    "Information: Suzy is 6'2\"\n",
    "\n",
    "Source: 2\n",
    "Information: Jeremiah is blonde\n",
    "\n",
    "Source: 3\n",
    "Information: Brian is 3 inches shorter than Suzy\"\"\"\n",
    "result = structured_llm.invoke(example_q)\n",
    "\n",
    "result"
   ],
   "id": "23a60d10d22d1361",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T16:03:33.309561Z",
     "start_time": "2024-07-29T16:03:33.303838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# 有了上面的demo，这里对文档做编号，实现和上面一样的效果\n",
    "\n",
    "def format_docs_with_id(docs: List[Document]) -> str:\n",
    "    \"\"\"这个函数是格式化输入文档，create_stuff_document 方法里面的实现和这个一样\"\"\"\n",
    "    formatted = [\n",
    "        f\"Source ID: {i}\\nArticle Title: {doc.metadata['title']}\\nArticle Snippet: {doc.page_content}\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs_with_id(x[\"context\"])))\n",
    "    | prompt\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ],
   "id": "44d4a0ed9754478e",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T16:03:52.352882Z",
     "start_time": "2024-07-29T16:03:48.410123Z"
    }
   },
   "cell_type": "code",
   "source": "result = chain.invoke({\"input\": \"向量数据库的优点是什么？\"})",
   "id": "1a1f37761e3fa744",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T16:04:16.797227Z",
     "start_time": "2024-07-29T16:04:16.791762Z"
    }
   },
   "cell_type": "code",
   "source": "print(result[\"answer\"])",
   "id": "eed15cb9de17464e",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "如上所述，返回了本次回答所引用的文档。。\n",
    "这个例子对引用文档的id这个功能体现的不太明显，因为retriever是在向量数据库中找的，向量数据库只是返回了文章的片段。\n",
    "但是如果retrieve是一个`WikipediaRetriever`的呢？对一个问题可能会搜索出不同的答案，这个时候document就不是一个片段，而是一整个文章，这个时候就体现出来了（本次回答引用了哪些文章）\n",
    "\n",
    "比如豆包的在线搜索\n",
    "![](../resource/img_17.png)"
   ],
   "id": "d8492c212ce8d912"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 返回引用的原文\n",
    "\n",
    "和上面实现的原理一致，这里让返回的结构更加的复杂了一下，增加了`quote`字段来返回引用的原文。\n",
    "\n",
    "一般在经过chunk之后，文本已经变得小了，这里让模型直接返回文本会让结果变得更加的清晰。（ps：这里我觉得只是存在chunk的情况下，否则文本过大，也会增加token的消耗。）\n"
   ],
   "id": "54b16ebbab661e7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T15:37:23.307110Z",
     "start_time": "2024-07-30T15:37:23.304094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "class Citation(BaseModel):\n",
    "    \n",
    "    source_id: int = Field(\n",
    "        description=\"回答这个问题所引用的文档id\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        description=\"从指定来源中引用的原文，证明答案的正确性。\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"根据提供的来源回答用户问题，并引用所使用的来源。\"\"\"\n",
    "\n",
    "    answer: str = Field(\n",
    "        description=\"根据提供的来源，回答用户的问题。\",\n",
    "    )\n",
    "    citations: List[Citation] = Field( description=\"从提供的来源中引用支持答案的内容。\"\n",
    "    )"
   ],
   "id": "8d54bab2d133f312",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T15:37:28.485176Z",
     "start_time": "2024-07-30T15:37:26.365599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(QuotedAnswer)\n",
    "\n",
    "example_q = \"\"\"What Brian's height?\n",
    "\n",
    "Source: 1\n",
    "Information: Suzy is 6'2\"\n",
    "\n",
    "Source: 2\n",
    "Information: Jeremiah is blonde\n",
    "\n",
    "Source: 3\n",
    "Information: Brian is 3 inches shorter than Suzy\"\"\"\n",
    "result = structured_llm.invoke(example_q)\n",
    "\n",
    "result"
   ],
   "id": "aa84db06d5258d73",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "按照既定要求，模型返回的数据中已经包含了原文。同样的，也可以将上面的代码修改成这个样子。",
   "id": "152d957cc670b4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 在retriever的时候如何做用户的隔离\n",
    "\n",
    "LangChain有很多的retriever，LangChain并没有封装这样的能力，隔离的底层实现还得是各个retriever的能力。\n",
    "官网：https://python.langchain.com/v0.2/docs/how_to/qa_per_user/\n",
    "\n",
    "下面的demo中处理一个场景，不同的用户有不同的知识库。"
   ],
   "id": "d1d3c87fc597036e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:11:24.896816Z",
     "start_time": "2024-07-30T16:11:20.533595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://guangzhengli.com/blog/zh/vector-database/\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(),collection_name=\"userA\")\n",
    "retrieverA = vectorstore.as_retriever()"
   ],
   "id": "40b916ebbdd5996",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:11:31.405501Z",
     "start_time": "2024-07-30T16:11:24.898315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用户B 加载的是 https://daliuchen.github.io/langchain-guide/content/section_10.html\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://daliuchen.github.io/langchain-guide/content/section_10.html\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(),collection_name=\"userB\")\n",
    "retrieverB = vectorstore.as_retriever()\n",
    "retrieverB.invoke(\"chat History是什么？\")"
   ],
   "id": "379834ebbfa380e9",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T16:11:38.977596Z",
     "start_time": "2024-07-30T16:11:36.400735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(retrieverA.invoke(\"向量数据库的特点是什么？\"))\n",
    "print(retrieverB.invoke(\"chat History是什么？\"))"
   ],
   "id": "401b699391c59f5c",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如上所示，在Embedding之后，将结果存储在数据库，我采用的方式是使用不同的`collection name`来做，这是隔离的一个基本思路，在实际中也是，让用户连接，构建chain的时候，就可以指定该用户所对应的`collection name`，实现数据隔离。",
   "id": "27a8070453b5c7ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 如何在提取时处理长文本\n",
    "文档：https://python.langchain.com/v0.2/docs/how_to/extraction_long_text/\n",
    "\n",
    "在处理文本的时候，会遇到文本太长超过llm的限制，有三种方式来处理\n",
    "1. 换一个模型（可能更加的简单且效果好）\n",
    "2. 将文本简单的分块，从每个块中提取内容\n",
    "3. 对文本做RAG，对每个块做index，和问题相关的块做提取\n",
    "\n",
    "下面的demo演示了2，3两个方法"
   ],
   "id": "724d88e70cab4536"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:03:17.561520Z",
     "start_time": "2024-07-31T15:03:17.199975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 准备\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "# Download the content\n",
    "response = requests.get(\"https://baike.baidu.com/item/比亚迪\")\n",
    "# Write it to a file\n",
    "with open(\"/tmp/car.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "# Load it with an HTML parser\n",
    "loader = BSHTMLLoader(\"/tmp/car.html\",bs_kwargs={\"features\": \"html\"})\n",
    "document = loader.load()[0]\n",
    "# Clean up code\n",
    "# Replace consecutive new lines with a single new line\n",
    "document.page_content = re.sub(\"\\n\\n+\", \"\\n\", document.page_content)"
   ],
   "id": "8cbce4d66ec204b3",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:03:30.245870Z",
     "start_time": "2024-07-31T15:03:30.243554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这是直接加载了一个网页，他是一个超长文本，\n",
    "print(len(document.page_content))"
   ],
   "id": "c30a3b2bad751959",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:03:33.963142Z",
     "start_time": "2024-07-31T15:03:33.832174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class KeyDevelopment(BaseModel):\n",
    "    \"\"\"比亚迪车型的相关信息\"\"\"\n",
    "\n",
    "    year: int = Field(description=\"车的上市年份.\")\n",
    "    car_name: str = Field(description=\"车的名字\")\n",
    "\n",
    "\n",
    "class ExtractionData(BaseModel):\n",
    "    \"\"\"比亚迪汽车车型的提取信息。\"\"\"\n",
    "    key_developments: List[KeyDevelopment]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你擅长提取文本中比亚迪车型发布的重要节点，如果提取不到重要信息，则不提取任何内容\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "22aea9be21a56885",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:04:46.222164Z",
     "start_time": "2024-07-31T15:04:22.602725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "extractor = prompt | llm.with_structured_output(\n",
    "    schema=ExtractionData,\n",
    "    include_raw=False,\n",
    ")\n",
    "extractor.invoke({\"text\":document.page_content})"
   ],
   "id": "7a6637e5943278fd",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "已经准备好了，下面开始干活",
   "id": "ab74dbad03f4689e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 直接分块提取\n",
    "\n",
    "这种方式比较简单，将各个结果分块，各自去调用llm，走上面的提取逻辑，再将结果合并"
   ],
   "id": "fa1e5c74e44acee4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:14:15.951843Z",
     "start_time": "2024-07-31T15:14:03.453268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    # Controls the size of each chunk\n",
    "    chunk_size=2000,\n",
    "    # Controls overlap between chunks\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(document.page_content)\n",
    "# Limit just to the first 3 chunks\n",
    "# so the code can be re-run quickly\n",
    "first_few = texts[:3]\n",
    "\n",
    "extractions = extractor.batch(\n",
    "    [{\"text\": text} for text in first_few],\n",
    "    {\"max_concurrency\": 5},  # limit the concurrency by passing max concurrency!\n",
    ")"
   ],
   "id": "815ef35d1632bd2e",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "合并结果",
   "id": "e642a792d9b1b844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:15:56.475341Z",
     "start_time": "2024-07-31T15:15:56.471385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "key_developments = []\n",
    "\n",
    "for extraction in extractions:\n",
    "    key_developments.extend(extraction.key_developments)\n",
    "\n",
    "key_developments[:10]"
   ],
   "id": "9c0ee73e760e72e5",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 对文本做RAG\n",
    "\n",
    "这种方式就是RAG，没什么新意可言。"
   ],
   "id": "6fa139284b60b5b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:18:59.308960Z",
     "start_time": "2024-07-31T15:18:50.547192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "texts = text_splitter.split_text(document.page_content)\n",
    "vectorstore = Chroma.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 1}\n",
    ")  # Only extract from first document"
   ],
   "id": "c384c61943b665f2",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:19:17.627404Z",
     "start_time": "2024-07-31T15:19:02.307121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_extractor = {\n",
    "    \"text\": retriever | (lambda docs: docs[0].page_content)  # fetch content of top doc\n",
    "} | extractor\n",
    "\n",
    "results = rag_extractor.invoke(\"比亚迪有什么车型？\")"
   ],
   "id": "8dd1ab7873b0ae21",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:19:19.665259Z",
     "start_time": "2024-07-31T15:19:19.662524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key_development in results.key_developments:\n",
    "    print(key_development)"
   ],
   "id": "5499707e10c8e68c",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 只是使用Prompt不适用函数调用来生成结构化数据\n",
    "\n",
    "有些模型不支持函数调用（tool calling），想要返回结构化调用只能通过Prompt来做\n",
    "如果有的模型指令跟随是比较好的，通过Prompt能输出结构化数据\n",
    "通过下面的步骤来完成\n",
    "\n",
    "- 指示 LLM 按照预期格式生成文本\n",
    "- 使用输出解析器将模型的响应结构化为所需的 Python 对象。\n",
    "\n",
    "\n",
    "这里需要使用`PydanticOutputParser`"
   ],
   "id": "cd6144452ade8f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:31:36.297171Z",
     "start_time": "2024-07-31T15:31:36.286872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"人的信息\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"人名\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"人的身高，单位M\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"文本中提取的关于人的所有信息\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    "    # partial的意思是copy一份，并且渲染一部分\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "print(\"format_instructions:\\n\",parser.get_format_instructions())\n",
    "print(\"*\"*50)\n",
    "print(prompt.invoke({\"query\": \"小明一米五，小高比小明高一米\"}).to_messages()[0])\n",
    "print(\"*\"*50)\n",
    "print(prompt.invoke({\"query\": \"小明一米五，小高比小明高一米\"}).to_messages()[1])\n",
    "print(\"*\"*50)"
   ],
   "id": "cf6bae20767e0f20",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上面是完整的Prompt，通过`pydantic`定义的model，调用`get_format_instructions`方法后，会自动生成json的定义，所以这里不需要手动来做。\n",
    "\n",
    "在将上面的promot传递给LLM后，LLM会返回既定要求的文本（json格式），这里需要手动做解析，如下 "
   ],
   "id": "39ddf4ec6c0ea91c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:35:00.585138Z",
     "start_time": "2024-07-31T15:35:00.580318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# 自定义\n",
    "## llm输出的值为 AIMessage\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"```json(.*?)```\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ],
   "id": "48c45f9b0de99b77",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T15:36:14.136369Z",
     "start_time": "2024-07-31T15:36:10.522134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"小明一米五，小高比小明高一米\"\n",
    "chain = prompt | llm | extract_json\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "a3033da840506716",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "查看LangSmith如下：\n",
    "![](../resource/img_18.png)\n",
    "\n",
    "done!"
   ],
   "id": "e709fa1535480aa9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
