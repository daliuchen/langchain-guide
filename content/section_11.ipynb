{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 结构化结果和工具调用\n",
    "\n",
    "## 结构化结果\n",
    "LLM返回结构化数据这一功能极具实用价值，例如从数据中提取接口参数，亦或是将数据存储于数据库。\n",
    "接下来介绍几种相关方法\n",
    "\n",
    "### with_structured_output方法\n",
    "这是模型所提供的原生 API 用于输出结构化数据，此方式是最为简单且可靠的。不管是工具调用、函数调用还是 json 模型，其底层实现均采用这种模式。\n",
    "有一部分模型并不支持这一方法，而支持的 LLM 如下所示：\n",
    "https://python.langchain.com/v0.2/docs/integrations/chat/"
   ],
   "id": "c63105bec726d60d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:36:10.950397Z",
     "start_time": "2024-07-24T15:36:05.862280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# 开启debug\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "eb36d8e81819b265",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Tell me a joke about cats\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [4.98s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_oXCnqxGmzQaP4OTEWvPghiKR\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"setup\\\":\\\"Why was the cat sitting on the computer?\\\",\\\"punchline\\\":\\\"To keep an eye on the mouse!\\\",\\\"rating\\\":8}\",\n",
      "                    \"name\": \"Joke\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 30,\n",
      "                \"prompt_tokens\": 99,\n",
      "                \"total_tokens\": 129\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b62ea0f7-7234-4afb-a84e-590aec10c601-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Joke\",\n",
      "                \"args\": {\n",
      "                  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "                  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "                  \"rating\": 8\n",
      "                },\n",
      "                \"id\": \"call_oXCnqxGmzQaP4OTEWvPghiKR\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 99,\n",
      "              \"output_tokens\": 30,\n",
      "              \"total_tokens\": 129\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 30,\n",
      "      \"prompt_tokens\": 99,\n",
      "      \"total_tokens\": 129\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [4.98s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以看到上面在模型的返回中`text`是没有值的，在`kwargs.additional_kwargs.tool_calls`中返回了json，按照既定格式要求返回了数据。LangSmith如下：\n",
    "\n",
    "![](../resource/img_13.png)\n",
    "可以看到，这里将模型转为了`函数调用`，所以，这里的实现还是 function tools,查看`with_structured_output`源码可以证明\n",
    "![](../resource/img_14.png)"
   ],
   "id": "17a29cb2b1088e15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "也可以将schema的定义通过json的形式传递来，这样返回值是dict",
   "id": "a23ea9229006895c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:58:15.143852Z",
     "start_time": "2024-07-24T13:58:12.017764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "6d94e1e61cf0e91e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Tell me a joke about cats\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.11s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_5tOAi7xyZHkyZDCdaFvP5V4A\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"setup\\\":\\\"Why was the cat sitting on the computer?\\\",\\\"punchline\\\":\\\"To keep an eye on the mouse!\\\",\\\"rating\\\":7}\",\n",
      "                    \"name\": \"joke\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 30,\n",
      "                \"prompt_tokens\": 98,\n",
      "                \"total_tokens\": 128\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-291959a5-8ac6-4ba6-9faa-c1b6c5bd13e2-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"joke\",\n",
      "                \"args\": {\n",
      "                  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "                  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "                  \"rating\": 7\n",
      "                },\n",
      "                \"id\": \"call_5tOAi7xyZHkyZDCdaFvP5V4A\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 98,\n",
      "              \"output_tokens\": 30,\n",
      "              \"total_tokens\": 128\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 30,\n",
      "      \"prompt_tokens\": 98,\n",
      "      \"total_tokens\": 128\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:JsonOutputKeyToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:JsonOutputKeyToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "  \"rating\": 7\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.12s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "  \"punchline\": \"To keep an eye on the mouse!\",\n",
      "  \"rating\": 7\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why was the cat sitting on the computer?',\n",
       " 'punchline': 'To keep an eye on the mouse!',\n",
       " 'rating': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e31081af085992a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 在多个返回值类型中选择\n",
    "\n",
    "可以规定多个类型，让模型在里面做选择，如下所示"
   ],
   "id": "9f38649bc5fc0a13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:50:43.006165Z",
     "start_time": "2024-07-24T15:50:40.252442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: Union[Joke, ConversationalResponse]\n",
    "\n",
    " \n",
    "structured_llm = llm.with_structured_output(Response)\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "\n",
    "# 选择了 Joke"
   ],
   "id": "9d6418f7e697291a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Tell me a joke about cats\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.74s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_TiUIhjXu2gXxqm0F8ZdRvOhr\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"output\\\":{\\\"setup\\\":\\\"Why was the cat sitting on the computer?\\\",\\\"punchline\\\":\\\"To keep an eye on the mouse!\\\",\\\"rating\\\":8}}\",\n",
      "                    \"name\": \"Response\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 32,\n",
      "                \"prompt_tokens\": 162,\n",
      "                \"total_tokens\": 194\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0fd3ad2f-fee9-447f-98b5-8457545fa418-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Response\",\n",
      "                \"args\": {\n",
      "                  \"output\": {\n",
      "                    \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "                    \"punchline\": \"To keep an eye on the mouse!\",\n",
      "                    \"rating\": 8\n",
      "                  }\n",
      "                },\n",
      "                \"id\": \"call_TiUIhjXu2gXxqm0F8ZdRvOhr\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 162,\n",
      "              \"output_tokens\": 32,\n",
      "              \"total_tokens\": 194\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 32,\n",
      "      \"prompt_tokens\": 162,\n",
      "      \"total_tokens\": 194\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [2.75s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=8))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:00:32.487734Z",
     "start_time": "2024-07-24T14:00:30.171617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 选择了ConversationalResponse\n",
    "structured_llm.invoke(\"How are you today?\")"
   ],
   "id": "b23125cfe1301fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How are you today?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How are you today?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.31s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_8sWTjCQLG5qIyr4cX2XKOFLm\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"output\\\":{\\\"response\\\":\\\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\\\"}}\",\n",
      "                    \"name\": \"Response\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 37,\n",
      "                \"prompt_tokens\": 161,\n",
      "                \"total_tokens\": 198\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0e800ba8-6bf8-4655-b35e-23de46b6d1d5-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Response\",\n",
      "                \"args\": {\n",
      "                  \"output\": {\n",
      "                    \"response\": \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\"\n",
      "                  }\n",
      "                },\n",
      "                \"id\": \"call_8sWTjCQLG5qIyr4cX2XKOFLm\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 161,\n",
      "              \"output_tokens\": 37,\n",
      "              \"total_tokens\": 198\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 37,\n",
      "      \"prompt_tokens\": 161,\n",
      "      \"total_tokens\": 198\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [2.31s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\"))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Streaming\n",
    "也支持流式输出"
   ],
   "id": "e3477936094013cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:04:50.325220Z",
     "start_time": "2024-07-24T14:04:47.447596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Response)\n",
    "set_debug(False)\n",
    "\n",
    "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
    "    print(chunk,type(chunk))"
   ],
   "id": "593caf639b99c82b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None) <class '__main__.Response'>\n",
      "output=Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=8) <class '__main__.Response'>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Few-shot prompting\n",
    "\n",
    "对于更复杂的模式，向提示添加少量示例非常有用。这可以通过几种方式来实现。\n",
    "最简单的方式是将Example直接添加到system中"
   ],
   "id": "a29c740b056b432b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:09:46.382193Z",
     "start_time": "2024-07-24T14:09:43.120584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "set_debug(True)\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n",
    "\n",
    "Here are some examples of jokes:\n",
    "\n",
    "example_user: Tell me a joke about planes\n",
    "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
    "\n",
    "example_user: Tell me another joke about planes\n",
    "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
    "\n",
    "example_user: Now about caterpillars\n",
    "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
   ],
   "id": "d6d956e00b6fdc32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"what's something funny about woodpeckers\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"what's something funny about woodpeckers\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a hilarious comedian. Your specialty is knock-knock jokes. Return a joke which has the setup (the response to \\\"Who's there?\\\") and the final punchline (the response to \\\"<setup> who?\\\").\\n\\nHere are some examples of jokes:\\n\\nexample_user: Tell me a joke about planes\\nexample_assistant: {\\\"setup\\\": \\\"Why don't planes ever get tired?\\\", \\\"punchline\\\": \\\"Because they have rest wings!\\\", \\\"rating\\\": 2}\\n\\nexample_user: Tell me another joke about planes\\nexample_assistant: {\\\"setup\\\": \\\"Cargo\\\", \\\"punchline\\\": \\\"Cargo 'vroom vroom', but planes go 'zoom zoom'!\\\", \\\"rating\\\": 10}\\n\\nexample_user: Now about caterpillars\\nexample_assistant: {\\\"setup\\\": \\\"Caterpillar\\\", \\\"punchline\\\": \\\"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\\\", \\\"rating\\\": 5}\\nHuman: what's something funny about woodpeckers\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.25s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_iFuNs8CnUjB8KymqPE0b69oY\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"output\\\":{\\\"setup\\\":\\\"Would\\\",\\\"punchline\\\":\\\"Would you believe woodpeckers are always board?\\\",\\\"rating\\\":8}}\",\n",
      "                    \"name\": \"Response\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 27,\n",
      "                \"prompt_tokens\": 360,\n",
      "                \"total_tokens\": 387\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7c2b9bdd-1cb8-49d6-8600-105e2e4b9723-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Response\",\n",
      "                \"args\": {\n",
      "                  \"output\": {\n",
      "                    \"setup\": \"Would\",\n",
      "                    \"punchline\": \"Would you believe woodpeckers are always board?\",\n",
      "                    \"rating\": 8\n",
      "                  }\n",
      "                },\n",
      "                \"id\": \"call_iFuNs8CnUjB8KymqPE0b69oY\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 360,\n",
      "              \"output_tokens\": 27,\n",
      "              \"total_tokens\": 387\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 27,\n",
      "      \"prompt_tokens\": 360,\n",
      "      \"total_tokens\": 387\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticToolsParser] [2ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.25s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(output=Joke(setup='Would', punchline='Would you believe woodpeckers are always board?', rating=8))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (Advanced) Specifying the method for structuring outputs\n",
    "\n",
    "是用`json_mode`的形式，需要在promot中指定JSON返回值，并且schema的key。\n",
    "`with_structured_output`支持两种模型，一种是`function_calling` 如上所述，一种是`json_mode`, 在`json_mode`下，不会将schema包装为function tools给llm，而是一个promot，所以需要在promot中规定返回的json格式。源码和LangSmith如下\n",
    "![](../resource/img_15.png)\n",
    "![](../resource/img_16.png)"
   ],
   "id": "6959d28ada761acd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:00:31.380207Z",
     "start_time": "2024-07-24T16:00:28.904709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")"
   ],
   "id": "5969a05d0ee82fb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.46s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n    \\\"setup\\\": \\\"Why was the cat sitting on the computer?\\\",\\n    \\\"punchline\\\": \\\"It wanted to keep an eye on the mouse!\\\"\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n    \\\"setup\\\": \\\"Why was the cat sitting on the computer?\\\",\\n    \\\"punchline\\\": \\\"It wanted to keep an eye on the mouse!\\\"\\n}\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 33,\n",
      "                \"prompt_tokens\": 28,\n",
      "                \"total_tokens\": 61\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-0ab71d90-7c5e-4196-b06c-f97f2f75ead4-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 28,\n",
      "              \"output_tokens\": 33,\n",
      "              \"total_tokens\": 61\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 33,\n",
      "      \"prompt_tokens\": 28,\n",
      "      \"total_tokens\": 61\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticOutputParser] [4ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [2.46s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the cat sitting on the computer?', punchline='It wanted to keep an eye on the mouse!', rating=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### (Advanced) Raw outputs\n",
    "\n",
    "LLMs 在生成结构化输出时并非尽善尽美，尤其是当模式趋于复杂的时候。您可以通过传递 include_raw=True 这一参数，以避免引发异常，并自行去处理原始输出。这样做将会改变输出的格式，其中会包含原始消息输出（raw）、解析后的值（若成功解析）（parsed）以及任何产生的错误（parsing_error）。"
   ],
   "id": "2e7679d41864e159"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:28:27.968313Z",
     "start_time": "2024-07-24T14:28:24.954923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Joke, include_raw=True)\n",
    "\n",
    "res = structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")\n",
    "print(res)"
   ],
   "id": "9df5ff8af88c817a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<raw> > llm:ChatOpenAI] [2.98s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_YEfiL7tL4oOmPnMLGuJytYZ6\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"setup\\\":\\\"Why was the cat sitting on the computer?\\\",\\\"punchline\\\":\\\"To keep an eye on the mouse!\\\"}\",\n",
      "                    \"name\": \"Joke\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 26,\n",
      "                \"prompt_tokens\": 114,\n",
      "                \"total_tokens\": 140\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e8947799-902d-452b-94ba-0ba14563ef49-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"Joke\",\n",
      "                \"args\": {\n",
      "                  \"setup\": \"Why was the cat sitting on the computer?\",\n",
      "                  \"punchline\": \"To keep an eye on the mouse!\"\n",
      "                },\n",
      "                \"id\": \"call_YEfiL7tL4oOmPnMLGuJytYZ6\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 114,\n",
      "              \"output_tokens\": 26,\n",
      "              \"total_tokens\": 140\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 26,\n",
      "      \"prompt_tokens\": 114,\n",
      "      \"total_tokens\": 140\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<raw>] [2.98s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence > parser:PydanticToolsParser] [0ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error> > chain:RunnableParallel<parsed,parsing_error>] [4ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks > chain:RunnableAssign<parsed,parsing_error>] [7ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableWithFallbacks] [10ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.00s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YEfiL7tL4oOmPnMLGuJytYZ6', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"To keep an eye on the mouse!\"}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 114, 'total_tokens': 140}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e8947799-902d-452b-94ba-0ba14563ef49-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'To keep an eye on the mouse!'}, 'id': 'call_YEfiL7tL4oOmPnMLGuJytYZ6'}], usage_metadata={'input_tokens': 114, 'output_tokens': 26, 'total_tokens': 140}), 'parsed': Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None), 'parsing_error': None}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:29:11.303645Z",
     "start_time": "2024-07-24T14:29:11.299944Z"
    }
   },
   "cell_type": "code",
   "source": "res.keys()",
   "id": "a26553b1bb35bf90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['raw', 'parsed', 'parsing_error'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:29:46.702710Z",
     "start_time": "2024-07-24T14:29:46.700242Z"
    }
   },
   "cell_type": "code",
   "source": "res[\"parsed\"]",
   "id": "1070b98d4c87fc9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 自己写promot，自己解析输出值\n",
    "\n",
    "并非所有模型都支持 .with_structured_output() 这一功能，原因在于并非所有模型都支持工具调用或 JSON 模式。对于这类模型，你的直接提示模型采用特定的格式，同时使用输出解析器从原始的模型输出里提取结构化的响应。\n",
    "\n",
    "##### PydanticOutputParser"
   ],
   "id": "715d55029083309f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:01.158717Z",
     "start_time": "2024-07-24T14:44:01.153657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ],
   "id": "633df1b2e4187bf",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:05.853566Z",
     "start_time": "2024-07-24T14:44:05.850342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt_template.invoke(query).to_string())"
   ],
   "id": "216a4c039a2ca0e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"Anna is 23 years old and she is 6 feet tall\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "System: Answer the user query. Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n",
      "```\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:11.750681Z",
     "start_time": "2024-07-24T14:44:08.516265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 代入llm\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "365d892432844285",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"query\": \"Anna is 23 years old and she is 6 feet tall\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"query\": \"Anna is 23 years old and she is 6 feet tall\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Answer the user query. Wrap the output in `json` tags\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"description\\\": \\\"Identifying information about all people in a text.\\\", \\\"properties\\\": {\\\"people\\\": {\\\"title\\\": \\\"People\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"$ref\\\": \\\"#/definitions/Person\\\"}}}, \\\"required\\\": [\\\"people\\\"], \\\"definitions\\\": {\\\"Person\\\": {\\\"title\\\": \\\"Person\\\", \\\"description\\\": \\\"Information about a person.\\\", \\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"name\\\": {\\\"title\\\": \\\"Name\\\", \\\"description\\\": \\\"The name of the person\\\", \\\"type\\\": \\\"string\\\"}, \\\"height_in_meters\\\": {\\\"title\\\": \\\"Height In Meters\\\", \\\"description\\\": \\\"The height of the person expressed in meters.\\\", \\\"type\\\": \\\"number\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"height_in_meters\\\"]}}}\\n```\\nHuman: Anna is 23 years old and she is 6 feet tall\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [3.22s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"people\\\": [\\n    {\\n      \\\"name\\\": \\\"Anna\\\",\\n      \\\"height_in_meters\\\": 1.83\\n    }\\n  ]\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"people\\\": [\\n    {\\n      \\\"name\\\": \\\"Anna\\\",\\n      \\\"height_in_meters\\\": 1.83\\n    }\\n  ]\\n}\\n```\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 36,\n",
      "                \"prompt_tokens\": 312,\n",
      "                \"total_tokens\": 348\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b270aaff-0e9d-4f24-8b9f-572940a3fa47-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 312,\n",
      "              \"output_tokens\": 36,\n",
      "              \"total_tokens\": 348\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 36,\n",
      "      \"prompt_tokens\": 312,\n",
      "      \"total_tokens\": 348\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:PydanticOutputParser] [4ms] Exiting Parser run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [3.23s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Anna', height_in_meters=1.83)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Custom Parsing\n",
    "\n",
    "还可以使用 LangChain 表达式语言（LCEL）创建自定义提示和解析器，通过一个普通函数来解析来自模型的输出。\n",
    "这种方式是llm刚开始的阶段，不推荐使用。"
   ],
   "id": "ef943b614dd78ea6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:44:49.427704Z",
     "start_time": "2024-07-24T14:44:49.421740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Output your answer as JSON that  \"\n",
    "            \"matches the given schema: ```json\\n{schema}\\n```. \"\n",
    "            \"Make sure to wrap the answer in ```json and ``` tags\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(schema=People.schema())\n",
    "\n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"```json(.*?)```\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ],
   "id": "78621524d3c15d32",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T14:45:21.079061Z",
     "start_time": "2024-07-24T14:45:21.076716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.format_prompt(query=query).to_string())"
   ],
   "id": "90d4a378f9774004",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Output your answer as JSON that  matches the given schema: ```json\n",
      "{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\n",
      "```. Make sure to wrap the answer in ```json and ``` tags\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:09:18.500548Z",
     "start_time": "2024-07-24T16:09:13.145728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | llm | extract_json\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "7efef5c3a9b92976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"query\": \"What is 3 * 12?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"query\": \"What is 3 * 12?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Answer the user query. Output your answer as JSON that  matches the given schema: ```json\\n{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\\n```. Make sure to wrap the answer in ```json and ``` tags\\nHuman: What is 3 * 12?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [5.34s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"title\\\": \\\"People\\\",\\n    \\\"description\\\": \\\"Identifying information about all people in a text.\\\",\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n        \\\"people\\\": {\\n            \\\"title\\\": \\\"People\\\",\\n            \\\"type\\\": \\\"array\\\",\\n            \\\"items\\\": {\\n                \\\"$ref\\\": \\\"#/definitions/Person\\\"\\n            }\\n        }\\n    },\\n    \\\"required\\\": [\\\"people\\\"],\\n    \\\"definitions\\\": {\\n        \\\"Person\\\": {\\n            \\\"title\\\": \\\"Person\\\",\\n            \\\"description\\\": \\\"Information about a person.\\\",\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"name\\\": {\\n                    \\\"title\\\": \\\"Name\\\",\\n                    \\\"description\\\": \\\"The name of the person\\\",\\n                    \\\"type\\\": \\\"string\\\"\\n                },\\n                \\\"height_in_meters\\\": {\\n                    \\\"title\\\": \\\"Height In Meters\\\",\\n                    \\\"description\\\": \\\"The height of the person expressed in meters.\\\",\\n                    \\\"type\\\": \\\"number\\\"\\n                }\\n            },\\n            \\\"required\\\": [\\\"name\\\", \\\"height_in_meters\\\"]\\n        }\\n    }\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n    \\\"title\\\": \\\"People\\\",\\n    \\\"description\\\": \\\"Identifying information about all people in a text.\\\",\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n        \\\"people\\\": {\\n            \\\"title\\\": \\\"People\\\",\\n            \\\"type\\\": \\\"array\\\",\\n            \\\"items\\\": {\\n                \\\"$ref\\\": \\\"#/definitions/Person\\\"\\n            }\\n        }\\n    },\\n    \\\"required\\\": [\\\"people\\\"],\\n    \\\"definitions\\\": {\\n        \\\"Person\\\": {\\n            \\\"title\\\": \\\"Person\\\",\\n            \\\"description\\\": \\\"Information about a person.\\\",\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"name\\\": {\\n                    \\\"title\\\": \\\"Name\\\",\\n                    \\\"description\\\": \\\"The name of the person\\\",\\n                    \\\"type\\\": \\\"string\\\"\\n                },\\n                \\\"height_in_meters\\\": {\\n                    \\\"title\\\": \\\"Height In Meters\\\",\\n                    \\\"description\\\": \\\"The height of the person expressed in meters.\\\",\\n                    \\\"type\\\": \\\"number\\\"\\n                }\\n            },\\n            \\\"required\\\": [\\\"name\\\", \\\"height_in_meters\\\"]\\n        }\\n    }\\n}\\n```\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 218,\n",
      "                \"prompt_tokens\": 221,\n",
      "                \"total_tokens\": 439\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-dbeea975-f4d7-4d94-b1b5-33198bcfd49d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 221,\n",
      "              \"output_tokens\": 218,\n",
      "              \"total_tokens\": 439\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 218,\n",
      "      \"prompt_tokens\": 221,\n",
      "      \"total_tokens\": 439\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:extract_json] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:extract_json] [2ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"title\": \"People\",\n",
      "      \"description\": \"Identifying information about all people in a text.\",\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"people\": {\n",
      "          \"title\": \"People\",\n",
      "          \"type\": \"array\",\n",
      "          \"items\": {\n",
      "            \"$ref\": \"#/definitions/Person\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"people\"\n",
      "      ],\n",
      "      \"definitions\": {\n",
      "        \"Person\": {\n",
      "          \"title\": \"Person\",\n",
      "          \"description\": \"Information about a person.\",\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"name\": {\n",
      "              \"title\": \"Name\",\n",
      "              \"description\": \"The name of the person\",\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"height_in_meters\": {\n",
      "              \"title\": \"Height In Meters\",\n",
      "              \"description\": \"The height of the person expressed in meters.\",\n",
      "              \"type\": \"number\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"name\",\n",
      "            \"height_in_meters\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [5.34s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"title\": \"People\",\n",
      "      \"description\": \"Identifying information about all people in a text.\",\n",
      "      \"type\": \"object\",\n",
      "      \"properties\": {\n",
      "        \"people\": {\n",
      "          \"title\": \"People\",\n",
      "          \"type\": \"array\",\n",
      "          \"items\": {\n",
      "            \"$ref\": \"#/definitions/Person\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"people\"\n",
      "      ],\n",
      "      \"definitions\": {\n",
      "        \"Person\": {\n",
      "          \"title\": \"Person\",\n",
      "          \"description\": \"Information about a person.\",\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"name\": {\n",
      "              \"title\": \"Name\",\n",
      "              \"description\": \"The name of the person\",\n",
      "              \"type\": \"string\"\n",
      "            },\n",
      "            \"height_in_meters\": {\n",
      "              \"title\": \"Height In Meters\",\n",
      "              \"description\": \"The height of the person expressed in meters.\",\n",
      "              \"type\": \"number\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"name\",\n",
      "            \"height_in_meters\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'People',\n",
       "  'description': 'Identifying information about all people in a text.',\n",
       "  'type': 'object',\n",
       "  'properties': {'people': {'title': 'People',\n",
       "    'type': 'array',\n",
       "    'items': {'$ref': '#/definitions/Person'}}},\n",
       "  'required': ['people'],\n",
       "  'definitions': {'Person': {'title': 'Person',\n",
       "    'description': 'Information about a person.',\n",
       "    'type': 'object',\n",
       "    'properties': {'name': {'title': 'Name',\n",
       "      'description': 'The name of the person',\n",
       "      'type': 'string'},\n",
       "     'height_in_meters': {'title': 'Height In Meters',\n",
       "      'description': 'The height of the person expressed in meters.',\n",
       "      'type': 'number'}},\n",
       "    'required': ['name', 'height_in_meters']}}}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 如何使用chat models来调用工具\n",
    "\n",
    "官方文档：\n",
    "https://python.langchain.com/v0.2/docs/how_to/custom_tools/#creating-tools-from-functions\n",
    "\n",
    "### 将tools传递给llm\n",
    "\n",
    "`@tool`用来定义工具， 在定义的时候需要通过类型参数，文档注释等等来声明工具的作用，名字，参数，返回只等，在默认状况下，此装饰器会把函数名当作工具名。"
   ],
   "id": "e859ee7db766aff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:20:29.241212Z",
     "start_time": "2024-07-24T16:20:29.233896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "print(type(add))\n",
    "print(add.name)\n",
    "print(add.args)\n",
    "print(add.description)"
   ],
   "id": "d83d3cf48ebaad26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.tools.StructuredTool'>\n",
      "add\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n",
      "Adds a and b.\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:18:34.941059Z",
     "start_time": "2024-07-24T16:18:32.000946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 声明工具，传递给llm\n",
    "tools = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "ai_msg = llm_with_tools.invoke(query)"
   ],
   "id": "cc2d1facd8783470",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is 3 * 12? Also, what is 11 + 49?\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[llm:ChatOpenAI] [2.93s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"tool_calls\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"tool_calls\": [\n",
      "                {\n",
      "                  \"id\": \"call_4snehkbyqHQUPEHwagoH89Af\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"a\\\": 3, \\\"b\\\": 12}\",\n",
      "                    \"name\": \"multiply\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                },\n",
      "                {\n",
      "                  \"id\": \"call_t0kjiyJ4repsarAB4k6BNaUA\",\n",
      "                  \"function\": {\n",
      "                    \"arguments\": \"{\\\"a\\\": 11, \\\"b\\\": 49}\",\n",
      "                    \"name\": \"add\"\n",
      "                  },\n",
      "                  \"type\": \"function\"\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 49,\n",
      "                \"prompt_tokens\": 88,\n",
      "                \"total_tokens\": 137\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"tool_calls\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-709c5ff3-9781-4e4d-b428-b4e6b98d10c0-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"multiply\",\n",
      "                \"args\": {\n",
      "                  \"a\": 3,\n",
      "                  \"b\": 12\n",
      "                },\n",
      "                \"id\": \"call_4snehkbyqHQUPEHwagoH89Af\"\n",
      "              },\n",
      "              {\n",
      "                \"name\": \"add\",\n",
      "                \"args\": {\n",
      "                  \"a\": 11,\n",
      "                  \"b\": 49\n",
      "                },\n",
      "                \"id\": \"call_t0kjiyJ4repsarAB4k6BNaUA\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 88,\n",
      "              \"output_tokens\": 49,\n",
      "              \"total_tokens\": 137\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 49,\n",
      "      \"prompt_tokens\": 88,\n",
      "      \"total_tokens\": 137\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**要注意：LLM本身没有调用函数的能力，他的返回值只是告诉我们需要调用函数，调用哪些函数，入参是什么。**\n",
    "这些信息都封装在`tool_calls`方法中"
   ],
   "id": "7cef66e83de847c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:20:05.519449Z",
     "start_time": "2024-07-24T16:20:05.516101Z"
    }
   },
   "cell_type": "code",
   "source": "ai_msg.tool_calls",
   "id": "ed4b1c5d1c680c28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_4snehkbyqHQUPEHwagoH89Af'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'call_t0kjiyJ4repsarAB4k6BNaUA'}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "说回到tool,他的类型是`StructuredTool`，他也是实现了`Runnable`接口，所以，他是可以直接通过`invoke`调用的， 可以直接在LCEL中使用的",
   "id": "5412e7175f3d9810"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:21:56.274001Z",
     "start_time": "2024-07-24T16:21:56.268584Z"
    }
   },
   "cell_type": "code",
   "source": "add.invoke({\"a\":1,\"b\":1})",
   "id": "214b40c8711d49a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[tool/start]\u001B[0m \u001B[1m[tool:add] Entering Tool run with input:\n",
      "\u001B[0m\"{'a': 1, 'b': 1}\"\n",
      "\u001B[36;1m\u001B[1;3m[tool/end]\u001B[0m \u001B[1m[tool:add] [1ms] Exiting Tool run with output:\n",
      "\u001B[0m\"2\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "okk，对于上面的结果，就可以找到对应的tool，将llm中的调用tool的参数塞进去调用就完事。",
   "id": "522d173e8a139e80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T15:23:01.687653Z",
     "start_time": "2024-07-24T15:23:01.684941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call[\"args\"])\n",
    "    print(tool_msg)"
   ],
   "id": "242fdbdd6b156ab8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[tool/start]\u001B[0m \u001B[1m[tool:multiply] Entering Tool run with input:\n",
      "\u001B[0m\"{'a': 3, 'b': 12}\"\n",
      "\u001B[36;1m\u001B[1;3m[tool/end]\u001B[0m \u001B[1m[tool:multiply] [0ms] Exiting Tool run with output:\n",
      "\u001B[0m\"36\"\n",
      "36\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "end",
   "id": "58ebc14b740f796a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
